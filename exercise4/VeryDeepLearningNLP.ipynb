{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "VeryDeepLearningNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fIKZUwB5SBHV"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gruppe3VDL/Gruppe3VDLws2019/blob/master/exercise4/VeryDeepLearningNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjM7ZrbNSBFc",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 5 (NLP): Very Deep Learning\n",
        "\n",
        "**Natural language processing (NLP)** is the ability of a computer program to understand human language as it is spoken. It involves a pipeline of steps and by the end of the exercise, we would be able to classify the sentiment of a given review as POSITIVE or NEGATIVE.\n",
        "\n",
        "\n",
        "Before starting, it is important to understand the need for RNNs and the lecture from Stanford is a must to see before starting the exercise:\n",
        "\n",
        "https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
        "\n",
        "When done, let's begin. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3mXT-ExTSgH",
        "colab_type": "code",
        "outputId": "69722604-27b7-41ed-8a1b-72c9948f1407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Pa3af2dM7k",
        "colab_type": "code",
        "outputId": "2e91fb64-0bb6-459f-beff-3b1bcd7e5741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/TUK/Very Deep Learning/Exercises/exercise4/\"\n",
        "%ls"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TUK/Very Deep Learning/Exercises/exercise4\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/        'VeryDeepLearningNLP (2).ipynb'\n",
            " results.csv   VeryDeepLearningNLP.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78l6-GBPSBFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this exercise, we will import libraries when needed so that we understand the need for it. \n",
        "# However, this is a bad practice and don't get used to it.\n",
        "import numpy as np\n",
        "\n",
        "# read data from reviews and labels file.\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews_ = f.readlines()\n",
        "\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZjxFmhtSBFh",
        "colab_type": "code",
        "outputId": "16bbb709-8675-4f53-8b72-5e5bee673dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# One of the most important task is to visualize data before starting with any ML task. \n",
        "for i in range(5):\n",
        "    print(labels[i] + \"\\t: \" + reviews_[i][:100] + \"...\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n",
            "\t: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life...\n",
            "negative\n",
            "\t: story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terr...\n",
            "positive\n",
            "\t: homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan...\n",
            "negative\n",
            "\t: airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belongin...\n",
            "positive\n",
            "\t: brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love sce...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CyjlabhSBFk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "We can see there are a lot of punctuation marks like fullstop(.), comma(,), new line (\\n) and so on and we need to remove it. \n",
        "\n",
        "Here is a list of all the punctuation marks that needs to be removed \n",
        "```\n",
        "(!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T7LCUwWSBFl",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Remove all the punctuation marks from the reviews.\n",
        "Many ways of doing it: Regex, Spacy, import punctuation from string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5V1B0WDSBFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make everything lower case to make the whole dataset even. \n",
        "reviews = ''.join(reviews_).lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi4TH5ojSBFq",
        "colab_type": "code",
        "outputId": "c66cbeab-ea88-4d4f-ab6c-3bc695355173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# complete the function below to remove punctuations and save it in no_punct_text\n",
        "def text_without_punct(reviews):\n",
        "    # make everything lowercase\n",
        "    reviews__ = reviews.lower()\n",
        "\n",
        "    # remove punctuation\n",
        "    punc = r\"[!\\\"#\\$%&\\'\\(\\)\\*\\+,-.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]\"\n",
        "    reviews__ = re.sub(punc, '', reviews__)\n",
        "\n",
        "    # remove stopwords\n",
        "    nonsense = set(stopwords.words('english')) \n",
        "    for stopword in nonsense:\n",
        "        reviews__ = reviews__.replace(f' {stopword} ', ' ')\n",
        "\n",
        "    # remove multiple spaces\n",
        "    reviews__ = re.sub(' +', ' ', reviews__)\n",
        "    return reviews__\n",
        "\n",
        "no_punct_text = text_without_punct(reviews)\n",
        "reviews_split = no_punct_text.split('\\n')\n",
        "\n",
        "# One of the most important task is to visualize data before starting with any ML task. \n",
        "for i in range(5):\n",
        "    print(labels[i] + \"\\t: \" + reviews_split[i][:100] + \"...\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "positive\n",
            "\t: bromwell high cartoon comedy ran time programs school life teachers years teaching profession lead b...\n",
            "negative\n",
            "\t: story man unnatural feelings pig starts opening scene terrific example absurd comedy formal orchestr...\n",
            "positive\n",
            "\t: homelessness houselessness george carlin stated issue years never plan help street considered human ...\n",
            "negative\n",
            "\t: airport starts brand new luxury plane loaded valuable paintings belonging rich businessman philip st...\n",
            "positive\n",
            "\t: brilliant acting lesley ann warren best dramatic hobo lady ever seen love scenes clothes warehouse s...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0G9FrYdSBFs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7bec5113-4be9-4287-edcd-2162b7cf9b8b"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# split the formatted no_punct_text into words\n",
        "def split_in_words(no_punct_text):\n",
        "    words = word_tokenize(no_punct_text)\n",
        "    return words\n",
        "\n",
        "words = split_in_words(no_punct_text)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulLy2_TSBFv",
        "colab_type": "code",
        "outputId": "c392b57d-88bb-4804-e4d8-603dad2cb222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# once you are done print the ten words that should yield the following output\n",
        "words[:10]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'ran',\n",
              " 'time',\n",
              " 'programs',\n",
              " 'school',\n",
              " 'life',\n",
              " 'teachers']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa15FCovSBFx",
        "colab_type": "code",
        "outputId": "7cafa8a9-1b4c-4fdf-d1be-cb42a83b972c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print the total length of the words\n",
        "len(words)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3110186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9dOcdP-SBFz",
        "colab_type": "code",
        "outputId": "c13d0b8e-4732-46c2-bcbd-d6581774905b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Total number of unique words\n",
        "len(set(words))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74037"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_mmvikOSBF3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next step is to create a vocabulary. This way every word is mapped to an integer number.\n",
        "```\n",
        "Example: 1: hello, 2: I, 3: am, 4: Robo and so on...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp7yCYs0SBF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets create a vocab out of it\n",
        "\n",
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Let's keep a count of all the words and let's see how many words are there. \n",
        "def word_count(words):\n",
        "    return Counter(words)\n",
        "\n",
        "counts = word_count(words)\n",
        "\n",
        "# remove words that only occur once (they are probably typos)\n",
        "for word in words:\n",
        "    if counts['word'] == 1:\n",
        "        words.remove(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZXdS0PBSBF7",
        "colab_type": "code",
        "outputId": "86a12f28-545f-4380-b7d5-d4e97ecd55e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# If you did everything correct, this is what you should get as output. \n",
        "print(counts['wonderful'])\n",
        "print(counts['bad'])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1658\n",
            "9308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlTkzNjKSBF9",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Word to Integer and Integer to word\n",
        "The task is to map every word to an integer value and then vice-versa. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZeE9neCSBF9",
        "colab_type": "code",
        "outputId": "909b3471-90be-409a-b37a-0e2856f2d788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# define a vocabulary for the words\n",
        "def vocabulary(counts):\n",
        "    vocab = sorted(list(counts.keys()))\n",
        "    return vocab\n",
        "\n",
        "vocab = vocabulary(counts)\n",
        "print(len(vocab))\n",
        "vocab[1]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aa'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGI-sNo3SBGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map each vocab word to an integer. Also, start the indexing with 1 as we will use \n",
        "# '0' for padding and we dont want to mix the two.\n",
        "def vocabulary_to_integer(vocab):\n",
        "    index = 1\n",
        "    vocab_to_int = dict()\n",
        "    for word in vocab:\n",
        "        vocab_to_int[word] = index\n",
        "        index += 1\n",
        "\n",
        "    return vocab_to_int\n",
        "\n",
        "vocab_to_int = vocabulary_to_integer(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P22mGThMSBGC",
        "colab_type": "code",
        "outputId": "14616124-1070-47c3-c26a-48bf1ea09c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# verify if the length is same and if 'and' is mapped to the correct integer value.\n",
        "print(len(vocab_to_int))\n",
        "vocab_to_int['aa']"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8BVmU-mSBGF",
        "colab_type": "text"
      },
      "source": [
        "Let's see what positve words in positive reviews we have and what we have in negative reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqKhTHJYSBGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_counts = Counter()\n",
        "negative_counts = Counter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNreK9HW3-fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(reviews_)):\n",
        "    if(labels[i] == 'positive\\n'):\n",
        "        for word in reviews_[i].split(\" \"):\n",
        "              positive_counts[word] += 1\n",
        "    else:\n",
        "        for word in reviews_[i].split(\" \"):\n",
        "              negative_counts[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgFBpuMWSBGJ",
        "colab_type": "code",
        "outputId": "6073b677-519c-41cc-8c12-201c1d2346f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zcYyA7dSBGM",
        "colab_type": "code",
        "outputId": "04644050-3448-40b0-aec2-fdc049e6a25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "positive_counts.most_common()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 537968),\n",
              " ('the', 173324),\n",
              " ('.', 159654),\n",
              " ('and', 89722),\n",
              " ('a', 83688),\n",
              " ('of', 76855),\n",
              " ('to', 66746),\n",
              " ('is', 57245),\n",
              " ('in', 50215),\n",
              " ('br', 49235),\n",
              " ('it', 48025),\n",
              " ('i', 40743),\n",
              " ('that', 35630),\n",
              " ('this', 35080),\n",
              " ('s', 33815),\n",
              " ('as', 26308),\n",
              " ('with', 23247),\n",
              " ('for', 22416),\n",
              " ('was', 21917),\n",
              " ('film', 20937),\n",
              " ('but', 20822),\n",
              " ('movie', 19074),\n",
              " ('his', 17227),\n",
              " ('on', 17008),\n",
              " ('you', 16681),\n",
              " ('he', 16282),\n",
              " ('are', 14807),\n",
              " ('not', 14272),\n",
              " ('t', 13720),\n",
              " ('one', 13655),\n",
              " ('have', 12587),\n",
              " ('\\n', 12500),\n",
              " ('be', 12416),\n",
              " ('by', 11997),\n",
              " ('all', 11942),\n",
              " ('who', 11464),\n",
              " ('an', 11294),\n",
              " ('at', 11234),\n",
              " ('from', 10767),\n",
              " ('her', 10474),\n",
              " ('they', 9895),\n",
              " ('has', 9186),\n",
              " ('so', 9154),\n",
              " ('like', 9038),\n",
              " ('about', 8313),\n",
              " ('very', 8305),\n",
              " ('out', 8134),\n",
              " ('there', 8057),\n",
              " ('she', 7779),\n",
              " ('what', 7737),\n",
              " ('or', 7732),\n",
              " ('good', 7720),\n",
              " ('more', 7521),\n",
              " ('when', 7456),\n",
              " ('some', 7441),\n",
              " ('if', 7285),\n",
              " ('just', 7152),\n",
              " ('can', 7001),\n",
              " ('story', 6780),\n",
              " ('time', 6515),\n",
              " ('my', 6488),\n",
              " ('great', 6419),\n",
              " ('well', 6405),\n",
              " ('up', 6321),\n",
              " ('which', 6267),\n",
              " ('their', 6107),\n",
              " ('see', 6026),\n",
              " ('also', 5550),\n",
              " ('we', 5531),\n",
              " ('really', 5476),\n",
              " ('would', 5400),\n",
              " ('will', 5218),\n",
              " ('me', 5167),\n",
              " ('had', 5148),\n",
              " ('only', 5137),\n",
              " ('him', 5018),\n",
              " ('even', 4964),\n",
              " ('most', 4864),\n",
              " ('other', 4858),\n",
              " ('were', 4782),\n",
              " ('first', 4755),\n",
              " ('than', 4736),\n",
              " ('much', 4685),\n",
              " ('its', 4622),\n",
              " ('no', 4574),\n",
              " ('into', 4544),\n",
              " ('people', 4479),\n",
              " ('best', 4319),\n",
              " ('love', 4301),\n",
              " ('get', 4272),\n",
              " ('how', 4213),\n",
              " ('life', 4199),\n",
              " ('been', 4189),\n",
              " ('because', 4079),\n",
              " ('way', 4036),\n",
              " ('do', 3941),\n",
              " ('made', 3823),\n",
              " ('films', 3813),\n",
              " ('them', 3805),\n",
              " ('after', 3800),\n",
              " ('many', 3766),\n",
              " ('two', 3733),\n",
              " ('too', 3659),\n",
              " ('think', 3655),\n",
              " ('movies', 3586),\n",
              " ('characters', 3560),\n",
              " ('character', 3514),\n",
              " ('don', 3468),\n",
              " ('man', 3460),\n",
              " ('show', 3432),\n",
              " ('watch', 3424),\n",
              " ('seen', 3414),\n",
              " ('then', 3358),\n",
              " ('little', 3341),\n",
              " ('still', 3340),\n",
              " ('make', 3303),\n",
              " ('could', 3237),\n",
              " ('never', 3226),\n",
              " ('being', 3217),\n",
              " ('where', 3173),\n",
              " ('does', 3069),\n",
              " ('over', 3017),\n",
              " ('any', 3002),\n",
              " ('while', 2899),\n",
              " ('know', 2833),\n",
              " ('did', 2790),\n",
              " ('years', 2758),\n",
              " ('here', 2740),\n",
              " ('ever', 2734),\n",
              " ('end', 2696),\n",
              " ('these', 2694),\n",
              " ('such', 2590),\n",
              " ('real', 2568),\n",
              " ('scene', 2567),\n",
              " ('back', 2547),\n",
              " ('those', 2485),\n",
              " ('though', 2475),\n",
              " ('off', 2463),\n",
              " ('new', 2458),\n",
              " ('your', 2453),\n",
              " ('go', 2440),\n",
              " ('acting', 2437),\n",
              " ('plot', 2432),\n",
              " ('world', 2429),\n",
              " ('scenes', 2427),\n",
              " ('say', 2414),\n",
              " ('through', 2409),\n",
              " ('makes', 2390),\n",
              " ('better', 2381),\n",
              " ('now', 2368),\n",
              " ('work', 2346),\n",
              " ('young', 2343),\n",
              " ('old', 2311),\n",
              " ('ve', 2307),\n",
              " ('find', 2272),\n",
              " ('both', 2248),\n",
              " ('before', 2177),\n",
              " ('us', 2162),\n",
              " ('again', 2158),\n",
              " ('series', 2153),\n",
              " ('quite', 2143),\n",
              " ('something', 2135),\n",
              " ('cast', 2133),\n",
              " ('should', 2121),\n",
              " ('part', 2098),\n",
              " ('always', 2088),\n",
              " ('lot', 2087),\n",
              " ('another', 2075),\n",
              " ('actors', 2047),\n",
              " ('director', 2040),\n",
              " ('family', 2032),\n",
              " ('between', 2016),\n",
              " ('own', 2016),\n",
              " ('m', 1998),\n",
              " ('may', 1997),\n",
              " ('same', 1972),\n",
              " ('role', 1967),\n",
              " ('watching', 1966),\n",
              " ('every', 1954),\n",
              " ('funny', 1953),\n",
              " ('doesn', 1935),\n",
              " ('performance', 1928),\n",
              " ('few', 1918),\n",
              " ('bad', 1907),\n",
              " ('look', 1900),\n",
              " ('re', 1884),\n",
              " ('why', 1855),\n",
              " ('things', 1849),\n",
              " ('times', 1832),\n",
              " ('big', 1815),\n",
              " ('however', 1795),\n",
              " ('actually', 1790),\n",
              " ('action', 1789),\n",
              " ('going', 1783),\n",
              " ('bit', 1757),\n",
              " ('comedy', 1742),\n",
              " ('down', 1740),\n",
              " ('music', 1738),\n",
              " ('must', 1728),\n",
              " ('take', 1709),\n",
              " ('saw', 1692),\n",
              " ('long', 1690),\n",
              " ('right', 1688),\n",
              " ('fun', 1686),\n",
              " ('fact', 1684),\n",
              " ('excellent', 1683),\n",
              " ('around', 1674),\n",
              " ('didn', 1672),\n",
              " ('without', 1671),\n",
              " ('thing', 1662),\n",
              " ('thought', 1639),\n",
              " ('got', 1635),\n",
              " ('each', 1630),\n",
              " ('day', 1614),\n",
              " ('feel', 1597),\n",
              " ('seems', 1596),\n",
              " ('come', 1594),\n",
              " ('done', 1586),\n",
              " ('beautiful', 1580),\n",
              " ('especially', 1572),\n",
              " ('played', 1571),\n",
              " ('almost', 1566),\n",
              " ('want', 1562),\n",
              " ('yet', 1556),\n",
              " ('give', 1553),\n",
              " ('pretty', 1549),\n",
              " ('last', 1543),\n",
              " ('since', 1519),\n",
              " ('different', 1504),\n",
              " ('although', 1501),\n",
              " ('gets', 1490),\n",
              " ('true', 1487),\n",
              " ('interesting', 1481),\n",
              " ('job', 1470),\n",
              " ('enough', 1455),\n",
              " ('our', 1454),\n",
              " ('shows', 1447),\n",
              " ('horror', 1441),\n",
              " ('woman', 1439),\n",
              " ('tv', 1400),\n",
              " ('probably', 1398),\n",
              " ('father', 1395),\n",
              " ('original', 1393),\n",
              " ('girl', 1390),\n",
              " ('point', 1379),\n",
              " ('plays', 1378),\n",
              " ('wonderful', 1372),\n",
              " ('far', 1358),\n",
              " ('course', 1358),\n",
              " ('john', 1350),\n",
              " ('rather', 1340),\n",
              " ('isn', 1328),\n",
              " ('ll', 1326),\n",
              " ('later', 1324),\n",
              " ('dvd', 1324),\n",
              " ('whole', 1310),\n",
              " ('war', 1310),\n",
              " ('d', 1307),\n",
              " ('found', 1306),\n",
              " ('away', 1306),\n",
              " ('screen', 1305),\n",
              " ('nothing', 1300),\n",
              " ('year', 1297),\n",
              " ('once', 1296),\n",
              " ('hard', 1294),\n",
              " ('together', 1280),\n",
              " ('set', 1277),\n",
              " ('am', 1277),\n",
              " ('having', 1266),\n",
              " ('making', 1265),\n",
              " ('place', 1263),\n",
              " ('might', 1260),\n",
              " ('comes', 1260),\n",
              " ('sure', 1253),\n",
              " ('american', 1248),\n",
              " ('play', 1245),\n",
              " ('kind', 1244),\n",
              " ('perfect', 1242),\n",
              " ('takes', 1242),\n",
              " ('performances', 1237),\n",
              " ('himself', 1230),\n",
              " ('worth', 1221),\n",
              " ('everyone', 1221),\n",
              " ('anyone', 1214),\n",
              " ('actor', 1203),\n",
              " ('three', 1201),\n",
              " ('wife', 1196),\n",
              " ('classic', 1192),\n",
              " ('goes', 1186),\n",
              " ('ending', 1178),\n",
              " ('version', 1168),\n",
              " ('star', 1149),\n",
              " ('enjoy', 1146),\n",
              " ('book', 1142),\n",
              " ('nice', 1132),\n",
              " ('everything', 1128),\n",
              " ('during', 1124),\n",
              " ('put', 1118),\n",
              " ('seeing', 1111),\n",
              " ('least', 1102),\n",
              " ('house', 1100),\n",
              " ('high', 1095),\n",
              " ('watched', 1094),\n",
              " ('loved', 1087),\n",
              " ('men', 1087),\n",
              " ('night', 1082),\n",
              " ('anything', 1075),\n",
              " ('believe', 1071),\n",
              " ('guy', 1071),\n",
              " ('top', 1063),\n",
              " ('amazing', 1058),\n",
              " ('hollywood', 1056),\n",
              " ('looking', 1053),\n",
              " ('main', 1044),\n",
              " ('definitely', 1043),\n",
              " ('gives', 1031),\n",
              " ('home', 1029),\n",
              " ('seem', 1028),\n",
              " ('episode', 1023),\n",
              " ('audience', 1020),\n",
              " ('sense', 1020),\n",
              " ('truly', 1017),\n",
              " ('special', 1011),\n",
              " ('second', 1009),\n",
              " ('short', 1009),\n",
              " ('fan', 1009),\n",
              " ('mind', 1005),\n",
              " ('human', 1001),\n",
              " ('recommend', 999),\n",
              " ('full', 996),\n",
              " ('black', 995),\n",
              " ('help', 991),\n",
              " ('along', 989),\n",
              " ('trying', 987),\n",
              " ('small', 986),\n",
              " ('death', 985),\n",
              " ('friends', 981),\n",
              " ('remember', 974),\n",
              " ('often', 970),\n",
              " ('said', 966),\n",
              " ('favorite', 962),\n",
              " ('heart', 959),\n",
              " ('early', 957),\n",
              " ('left', 956),\n",
              " ('until', 955),\n",
              " ('script', 954),\n",
              " ('let', 954),\n",
              " ('maybe', 937),\n",
              " ('today', 936),\n",
              " ('live', 934),\n",
              " ('less', 934),\n",
              " ('moments', 933),\n",
              " ('others', 929),\n",
              " ('brilliant', 926),\n",
              " ('shot', 925),\n",
              " ('liked', 923),\n",
              " ('become', 916),\n",
              " ('won', 915),\n",
              " ('used', 910),\n",
              " ('style', 907),\n",
              " ('mother', 895),\n",
              " ('lives', 894),\n",
              " ('came', 893),\n",
              " ('stars', 890),\n",
              " ('cinema', 889),\n",
              " ('looks', 885),\n",
              " ('perhaps', 884),\n",
              " ('read', 882),\n",
              " ('enjoyed', 879),\n",
              " ('boy', 875),\n",
              " ('drama', 873),\n",
              " ('highly', 871),\n",
              " ('given', 870),\n",
              " ('playing', 867),\n",
              " ('use', 864),\n",
              " ('next', 859),\n",
              " ('women', 858),\n",
              " ('fine', 857),\n",
              " ('effects', 856),\n",
              " ('kids', 854),\n",
              " ('entertaining', 853),\n",
              " ('need', 852),\n",
              " ('line', 850),\n",
              " ('works', 848),\n",
              " ('someone', 847),\n",
              " ('mr', 836),\n",
              " ('simply', 835),\n",
              " ('picture', 833),\n",
              " ('children', 833),\n",
              " ('face', 831),\n",
              " ('keep', 831),\n",
              " ('friend', 831),\n",
              " ('dark', 830),\n",
              " ('overall', 828),\n",
              " ('certainly', 828),\n",
              " ('minutes', 827),\n",
              " ('wasn', 824),\n",
              " ('history', 822),\n",
              " ('finally', 820),\n",
              " ('couple', 816),\n",
              " ('against', 815),\n",
              " ('son', 809),\n",
              " ('understand', 808),\n",
              " ('lost', 807),\n",
              " ('michael', 805),\n",
              " ('else', 801),\n",
              " ('throughout', 798),\n",
              " ('fans', 797),\n",
              " ('city', 792),\n",
              " ('reason', 789),\n",
              " ('written', 787),\n",
              " ('production', 787),\n",
              " ('several', 784),\n",
              " ('school', 783),\n",
              " ('based', 781),\n",
              " ('rest', 781),\n",
              " ('try', 780),\n",
              " ('dead', 776),\n",
              " ('hope', 775),\n",
              " ('strong', 768),\n",
              " ('white', 765),\n",
              " ('tell', 759),\n",
              " ('itself', 758),\n",
              " ('half', 753),\n",
              " ('person', 749),\n",
              " ('sometimes', 746),\n",
              " ('past', 744),\n",
              " ('start', 744),\n",
              " ('genre', 743),\n",
              " ('beginning', 739),\n",
              " ('final', 739),\n",
              " ('town', 738),\n",
              " ('art', 734),\n",
              " ('humor', 732),\n",
              " ('game', 732),\n",
              " ('yes', 731),\n",
              " ('idea', 731),\n",
              " ('late', 730),\n",
              " ('becomes', 729),\n",
              " ('despite', 729),\n",
              " ('able', 726),\n",
              " ('case', 726),\n",
              " ('money', 723),\n",
              " ('child', 721),\n",
              " ('completely', 721),\n",
              " ('side', 719),\n",
              " ('camera', 716),\n",
              " ('getting', 714),\n",
              " ('instead', 712),\n",
              " ('soon', 702),\n",
              " ('under', 700),\n",
              " ('viewer', 699),\n",
              " ('age', 697),\n",
              " ('days', 696),\n",
              " ('stories', 696),\n",
              " ('felt', 694),\n",
              " ('simple', 694),\n",
              " ('roles', 693),\n",
              " ('video', 688),\n",
              " ('name', 683),\n",
              " ('either', 683),\n",
              " ('doing', 677),\n",
              " ('turns', 674),\n",
              " ('wants', 671),\n",
              " ('close', 671),\n",
              " ('title', 669),\n",
              " ('wrong', 668),\n",
              " ('went', 666),\n",
              " ('james', 665),\n",
              " ('evil', 659),\n",
              " ('budget', 657),\n",
              " ('episodes', 657),\n",
              " ('relationship', 655),\n",
              " ('fantastic', 653),\n",
              " ('piece', 653),\n",
              " ('david', 651),\n",
              " ('turn', 648),\n",
              " ('murder', 646),\n",
              " ('parts', 645),\n",
              " ('brother', 644),\n",
              " ('absolutely', 643),\n",
              " ('head', 643),\n",
              " ('experience', 642),\n",
              " ('eyes', 641),\n",
              " ('sex', 638),\n",
              " ('direction', 637),\n",
              " ('called', 637),\n",
              " ('directed', 636),\n",
              " ('lines', 634),\n",
              " ('behind', 633),\n",
              " ('sort', 632),\n",
              " ('actress', 631),\n",
              " ('lead', 630),\n",
              " ('oscar', 628),\n",
              " ('including', 627),\n",
              " ('example', 627),\n",
              " ('known', 625),\n",
              " ('musical', 625),\n",
              " ('chance', 621),\n",
              " ('score', 620),\n",
              " ('already', 619),\n",
              " ('feeling', 619),\n",
              " ('hit', 619),\n",
              " ('voice', 615),\n",
              " ('moment', 612),\n",
              " ('living', 612),\n",
              " ('low', 610),\n",
              " ('supporting', 610),\n",
              " ('ago', 609),\n",
              " ('themselves', 608),\n",
              " ('reality', 605),\n",
              " ('hilarious', 605),\n",
              " ('jack', 604),\n",
              " ('told', 603),\n",
              " ('hand', 601),\n",
              " ('quality', 600),\n",
              " ('moving', 600),\n",
              " ('dialogue', 600),\n",
              " ('song', 599),\n",
              " ('happy', 599),\n",
              " ('matter', 598),\n",
              " ('paul', 598),\n",
              " ('light', 594),\n",
              " ('future', 593),\n",
              " ('entire', 592),\n",
              " ('finds', 591),\n",
              " ('gave', 589),\n",
              " ('laugh', 587),\n",
              " ('released', 586),\n",
              " ('expect', 584),\n",
              " ('fight', 581),\n",
              " ('particularly', 580),\n",
              " ('cinematography', 579),\n",
              " ('police', 579),\n",
              " ('whose', 578),\n",
              " ('type', 578),\n",
              " ('sound', 578),\n",
              " ('view', 573),\n",
              " ('enjoyable', 573),\n",
              " ('number', 572),\n",
              " ('romantic', 572),\n",
              " ('husband', 572),\n",
              " ('daughter', 572),\n",
              " ('documentary', 571),\n",
              " ('self', 570),\n",
              " ('superb', 569),\n",
              " ('modern', 569),\n",
              " ('took', 569),\n",
              " ('robert', 569),\n",
              " ('mean', 566),\n",
              " ('shown', 563),\n",
              " ('coming', 561),\n",
              " ('important', 560),\n",
              " ('king', 559),\n",
              " ('leave', 559),\n",
              " ('change', 558),\n",
              " ('somewhat', 555),\n",
              " ('wanted', 555),\n",
              " ('tells', 554),\n",
              " ('events', 552),\n",
              " ('run', 552),\n",
              " ('career', 552),\n",
              " ('country', 552),\n",
              " ('heard', 550),\n",
              " ('season', 550),\n",
              " ('greatest', 549),\n",
              " ('girls', 549),\n",
              " ('etc', 547),\n",
              " ('care', 546),\n",
              " ('starts', 545),\n",
              " ('english', 542),\n",
              " ('killer', 541),\n",
              " ('tale', 540),\n",
              " ('guys', 540),\n",
              " ('totally', 540),\n",
              " ('animation', 540),\n",
              " ('usual', 539),\n",
              " ('miss', 535),\n",
              " ('opinion', 535),\n",
              " ('easy', 531),\n",
              " ('violence', 531),\n",
              " ('songs', 530),\n",
              " ('british', 528),\n",
              " ('says', 526),\n",
              " ('realistic', 525),\n",
              " ('writing', 524),\n",
              " ('writer', 522),\n",
              " ('act', 522),\n",
              " ('comic', 521),\n",
              " ('thriller', 519),\n",
              " ('television', 517),\n",
              " ('power', 516),\n",
              " ('ones', 515),\n",
              " ('kid', 514),\n",
              " ('york', 513),\n",
              " ('novel', 513),\n",
              " ('alone', 512),\n",
              " ('problem', 512),\n",
              " ('attention', 509),\n",
              " ('involved', 508),\n",
              " ('kill', 507),\n",
              " ('extremely', 507),\n",
              " ('seemed', 506),\n",
              " ('hero', 505),\n",
              " ('french', 505),\n",
              " ('rock', 504),\n",
              " ('stuff', 501),\n",
              " ('wish', 499),\n",
              " ('begins', 498),\n",
              " ('taken', 497),\n",
              " ('sad', 497),\n",
              " ('ways', 496),\n",
              " ('richard', 495),\n",
              " ('knows', 494),\n",
              " ('atmosphere', 493),\n",
              " ('similar', 491),\n",
              " ('surprised', 491),\n",
              " ('taking', 491),\n",
              " ('car', 491),\n",
              " ('george', 490),\n",
              " ('perfectly', 490),\n",
              " ('across', 489),\n",
              " ('team', 489),\n",
              " ('eye', 489),\n",
              " ('sequence', 489),\n",
              " ('room', 488),\n",
              " ('due', 488),\n",
              " ('among', 488),\n",
              " ('serious', 488),\n",
              " ('powerful', 488),\n",
              " ('strange', 487),\n",
              " ('order', 487),\n",
              " ('cannot', 487),\n",
              " ('b', 487),\n",
              " ('beauty', 486),\n",
              " ('famous', 485),\n",
              " ('happened', 484),\n",
              " ('tries', 484),\n",
              " ('herself', 484),\n",
              " ('myself', 484),\n",
              " ('class', 483),\n",
              " ('four', 482),\n",
              " ('cool', 481),\n",
              " ('release', 479),\n",
              " ('anyway', 479),\n",
              " ('theme', 479),\n",
              " ('opening', 478),\n",
              " ('entertainment', 477),\n",
              " ('slow', 475),\n",
              " ('ends', 475),\n",
              " ('unique', 475),\n",
              " ('exactly', 475),\n",
              " ('easily', 474),\n",
              " ('level', 474),\n",
              " ('o', 474),\n",
              " ('red', 474),\n",
              " ('interest', 472),\n",
              " ('happen', 471),\n",
              " ('crime', 470),\n",
              " ('viewing', 468),\n",
              " ('sets', 467),\n",
              " ('memorable', 467),\n",
              " ('stop', 466),\n",
              " ('group', 466),\n",
              " ('problems', 463),\n",
              " ('dance', 463),\n",
              " ('working', 463),\n",
              " ('sister', 463),\n",
              " ('message', 463),\n",
              " ('knew', 462),\n",
              " ('mystery', 461),\n",
              " ('nature', 461),\n",
              " ('bring', 460),\n",
              " ('believable', 459),\n",
              " ('thinking', 459),\n",
              " ('brought', 459),\n",
              " ('mostly', 458),\n",
              " ('disney', 457),\n",
              " ('couldn', 457),\n",
              " ('society', 456),\n",
              " ('lady', 455),\n",
              " ('within', 455),\n",
              " ('blood', 454),\n",
              " ('parents', 453),\n",
              " ('upon', 453),\n",
              " ('viewers', 453),\n",
              " ('meets', 452),\n",
              " ('form', 452),\n",
              " ('peter', 452),\n",
              " ('tom', 452),\n",
              " ('usually', 452),\n",
              " ('soundtrack', 452),\n",
              " ('local', 450),\n",
              " ('certain', 448),\n",
              " ('follow', 448),\n",
              " ('whether', 447),\n",
              " ('possible', 446),\n",
              " ('emotional', 445),\n",
              " ('killed', 444),\n",
              " ('above', 444),\n",
              " ('de', 444),\n",
              " ('god', 443),\n",
              " ('middle', 443),\n",
              " ('needs', 442),\n",
              " ('happens', 442),\n",
              " ('flick', 442),\n",
              " ('masterpiece', 441),\n",
              " ('period', 440),\n",
              " ('major', 440),\n",
              " ('named', 439),\n",
              " ('haven', 439),\n",
              " ('particular', 438),\n",
              " ('th', 438),\n",
              " ('earth', 437),\n",
              " ('feature', 437),\n",
              " ('stand', 436),\n",
              " ('words', 435),\n",
              " ('typical', 435),\n",
              " ('elements', 433),\n",
              " ('obviously', 433),\n",
              " ('romance', 431),\n",
              " ('jane', 430),\n",
              " ('yourself', 427),\n",
              " ('showing', 427),\n",
              " ('brings', 426),\n",
              " ('fantasy', 426),\n",
              " ('guess', 423),\n",
              " ('america', 423),\n",
              " ('unfortunately', 422),\n",
              " ('huge', 422),\n",
              " ('indeed', 421),\n",
              " ('running', 421),\n",
              " ('talent', 420),\n",
              " ('stage', 419),\n",
              " ('started', 418),\n",
              " ('leads', 417),\n",
              " ('sweet', 417),\n",
              " ('japanese', 417),\n",
              " ('poor', 416),\n",
              " ('deal', 416),\n",
              " ('incredible', 413),\n",
              " ('personal', 413),\n",
              " ('fast', 412),\n",
              " ('became', 410),\n",
              " ('deep', 410),\n",
              " ('hours', 409),\n",
              " ('giving', 408),\n",
              " ('nearly', 408),\n",
              " ('dream', 408),\n",
              " ('clearly', 407),\n",
              " ('turned', 407),\n",
              " ('obvious', 406),\n",
              " ('near', 406),\n",
              " ('cut', 405),\n",
              " ('surprise', 405),\n",
              " ('era', 404),\n",
              " ('body', 404),\n",
              " ('hour', 403),\n",
              " ('female', 403),\n",
              " ('five', 403),\n",
              " ('note', 399),\n",
              " ('learn', 398),\n",
              " ('truth', 398),\n",
              " ('except', 397),\n",
              " ('feels', 397),\n",
              " ('match', 397),\n",
              " ('tony', 397),\n",
              " ('filmed', 394),\n",
              " ('clear', 394),\n",
              " ('complete', 394),\n",
              " ('street', 393),\n",
              " ('eventually', 393),\n",
              " ('keeps', 393),\n",
              " ('older', 393),\n",
              " ('lots', 393),\n",
              " ('buy', 392),\n",
              " ('william', 391),\n",
              " ('stewart', 391),\n",
              " ('fall', 390),\n",
              " ('joe', 390),\n",
              " ('meet', 390),\n",
              " ('unlike', 389),\n",
              " ('talking', 389),\n",
              " ('shots', 389),\n",
              " ('rating', 389),\n",
              " ('difficult', 389),\n",
              " ('dramatic', 388),\n",
              " ('means', 388),\n",
              " ('situation', 386),\n",
              " ('wonder', 386),\n",
              " ('present', 386),\n",
              " ('appears', 386),\n",
              " ('subject', 386),\n",
              " ('comments', 385),\n",
              " ('general', 383),\n",
              " ('sequences', 383),\n",
              " ('lee', 383),\n",
              " ('points', 382),\n",
              " ('earlier', 382),\n",
              " ('gone', 379),\n",
              " ('check', 379),\n",
              " ('suspense', 378),\n",
              " ('recommended', 378),\n",
              " ('ten', 378),\n",
              " ('third', 377),\n",
              " ('business', 377),\n",
              " ('talk', 375),\n",
              " ('leaves', 375),\n",
              " ('beyond', 375),\n",
              " ('portrayal', 374),\n",
              " ('beautifully', 373),\n",
              " ('single', 372),\n",
              " ('bill', 372),\n",
              " ('plenty', 371),\n",
              " ('word', 371),\n",
              " ('whom', 370),\n",
              " ('falls', 370),\n",
              " ('scary', 369),\n",
              " ('non', 369),\n",
              " ('figure', 369),\n",
              " ('battle', 369),\n",
              " ('using', 368),\n",
              " ('return', 368),\n",
              " ('doubt', 367),\n",
              " ('add', 367),\n",
              " ('hear', 366),\n",
              " ('solid', 366),\n",
              " ('success', 366),\n",
              " ('jokes', 365),\n",
              " ('oh', 365),\n",
              " ('touching', 365),\n",
              " ('political', 365),\n",
              " ('hell', 364),\n",
              " ('awesome', 364),\n",
              " ('boys', 364),\n",
              " ('sexual', 362),\n",
              " ('recently', 362),\n",
              " ('dog', 362),\n",
              " ('please', 361),\n",
              " ('wouldn', 361),\n",
              " ('straight', 361),\n",
              " ('features', 361),\n",
              " ('forget', 360),\n",
              " ('setting', 360),\n",
              " ('lack', 360),\n",
              " ('married', 359),\n",
              " ('mark', 359),\n",
              " ('social', 357),\n",
              " ('interested', 356),\n",
              " ('adventure', 356),\n",
              " ('actual', 355),\n",
              " ('terrific', 355),\n",
              " ('sees', 355),\n",
              " ('brothers', 355),\n",
              " ('move', 354),\n",
              " ('call', 354),\n",
              " ('various', 353),\n",
              " ('theater', 353),\n",
              " ('dr', 353),\n",
              " ('animated', 352),\n",
              " ('western', 351),\n",
              " ('baby', 350),\n",
              " ('space', 350),\n",
              " ('leading', 348),\n",
              " ('disappointed', 348),\n",
              " ('portrayed', 346),\n",
              " ('aren', 346),\n",
              " ('screenplay', 345),\n",
              " ('smith', 345),\n",
              " ('towards', 344),\n",
              " ('hate', 344),\n",
              " ('noir', 343),\n",
              " ('outstanding', 342),\n",
              " ('decent', 342),\n",
              " ('kelly', 342),\n",
              " ('directors', 341),\n",
              " ('journey', 341),\n",
              " ('none', 340),\n",
              " ('looked', 340),\n",
              " ('effective', 340),\n",
              " ('storyline', 339),\n",
              " ('caught', 339),\n",
              " ('sci', 339),\n",
              " ('fi', 339),\n",
              " ('cold', 339),\n",
              " ('mary', 339),\n",
              " ('rich', 338),\n",
              " ('charming', 338),\n",
              " ('popular', 337),\n",
              " ('rare', 337),\n",
              " ('manages', 337),\n",
              " ('harry', 337),\n",
              " ('spirit', 336),\n",
              " ('appreciate', 335),\n",
              " ('open', 335),\n",
              " ('moves', 334),\n",
              " ('basically', 334),\n",
              " ('acted', 334),\n",
              " ('inside', 333),\n",
              " ('boring', 333),\n",
              " ('century', 333),\n",
              " ('mention', 333),\n",
              " ('deserves', 333),\n",
              " ('subtle', 333),\n",
              " ('pace', 333),\n",
              " ('familiar', 332),\n",
              " ('background', 332),\n",
              " ('ben', 331),\n",
              " ('creepy', 330),\n",
              " ('supposed', 330),\n",
              " ('secret', 329),\n",
              " ('die', 328),\n",
              " ('jim', 328),\n",
              " ('question', 327),\n",
              " ('effect', 327),\n",
              " ('natural', 327),\n",
              " ('impressive', 326),\n",
              " ('rate', 326),\n",
              " ('language', 326),\n",
              " ('saying', 325),\n",
              " ('intelligent', 325),\n",
              " ('telling', 324),\n",
              " ('realize', 324),\n",
              " ('material', 324),\n",
              " ('scott', 324),\n",
              " ('singing', 323),\n",
              " ('dancing', 322),\n",
              " ('visual', 321),\n",
              " ('adult', 321),\n",
              " ('imagine', 321),\n",
              " ('kept', 320),\n",
              " ('office', 320),\n",
              " ('uses', 319),\n",
              " ('pure', 318),\n",
              " ('wait', 318),\n",
              " ('stunning', 318),\n",
              " ('review', 317),\n",
              " ('previous', 317),\n",
              " ('copy', 317),\n",
              " ('seriously', 317),\n",
              " ('reading', 316),\n",
              " ('create', 316),\n",
              " ('hot', 316),\n",
              " ('created', 316),\n",
              " ('magic', 316),\n",
              " ('somehow', 316),\n",
              " ('stay', 315),\n",
              " ('attempt', 315),\n",
              " ('escape', 315),\n",
              " ('crazy', 315),\n",
              " ('air', 315),\n",
              " ('frank', 315),\n",
              " ('hands', 314),\n",
              " ('filled', 313),\n",
              " ('expected', 312),\n",
              " ('average', 312),\n",
              " ('surprisingly', 312),\n",
              " ('complex', 311),\n",
              " ('quickly', 310),\n",
              " ('successful', 310),\n",
              " ('studio', 310),\n",
              " ('plus', 309),\n",
              " ('male', 309),\n",
              " ('co', 307),\n",
              " ('images', 306),\n",
              " ('casting', 306),\n",
              " ('following', 306),\n",
              " ('minute', 306),\n",
              " ('exciting', 306),\n",
              " ('members', 305),\n",
              " ('follows', 305),\n",
              " ('themes', 305),\n",
              " ('german', 305),\n",
              " ('reasons', 305),\n",
              " ('e', 305),\n",
              " ('touch', 304),\n",
              " ('edge', 304),\n",
              " ('free', 304),\n",
              " ('cute', 304),\n",
              " ('genius', 304),\n",
              " ('outside', 303),\n",
              " ('reviews', 302),\n",
              " ('admit', 302),\n",
              " ('ok', 302),\n",
              " ('younger', 302),\n",
              " ('fighting', 301),\n",
              " ('odd', 301),\n",
              " ('master', 301),\n",
              " ('recent', 300),\n",
              " ('thanks', 300),\n",
              " ('break', 300),\n",
              " ('comment', 300),\n",
              " ('apart', 299),\n",
              " ('emotions', 298),\n",
              " ('lovely', 298),\n",
              " ('begin', 298),\n",
              " ('doctor', 297),\n",
              " ('party', 297),\n",
              " ('italian', 297),\n",
              " ('la', 296),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l8dGeRsSBGO",
        "colab_type": "code",
        "outputId": "64b03e94-6d09-4a80-ac73-ef14fb3f70cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "negative_counts.most_common()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 548962),\n",
              " ('.', 167538),\n",
              " ('the', 163389),\n",
              " ('a', 79321),\n",
              " ('and', 74385),\n",
              " ('of', 69009),\n",
              " ('to', 68974),\n",
              " ('br', 52637),\n",
              " ('is', 50083),\n",
              " ('it', 48327),\n",
              " ('i', 46880),\n",
              " ('in', 43753),\n",
              " ('this', 40920),\n",
              " ('that', 37615),\n",
              " ('s', 31546),\n",
              " ('was', 26291),\n",
              " ('movie', 24965),\n",
              " ('for', 21927),\n",
              " ('but', 21781),\n",
              " ('with', 20878),\n",
              " ('as', 20625),\n",
              " ('t', 20361),\n",
              " ('film', 19218),\n",
              " ('you', 17549),\n",
              " ('on', 17192),\n",
              " ('not', 16354),\n",
              " ('have', 15144),\n",
              " ('are', 14623),\n",
              " ('be', 14541),\n",
              " ('he', 13856),\n",
              " ('one', 13134),\n",
              " ('they', 13011),\n",
              " ('\\n', 12500),\n",
              " ('at', 12279),\n",
              " ('his', 12147),\n",
              " ('all', 12036),\n",
              " ('so', 11463),\n",
              " ('like', 11238),\n",
              " ('there', 10775),\n",
              " ('just', 10619),\n",
              " ('by', 10549),\n",
              " ('or', 10272),\n",
              " ('an', 10266),\n",
              " ('who', 9969),\n",
              " ('from', 9731),\n",
              " ('if', 9518),\n",
              " ('about', 9061),\n",
              " ('out', 8979),\n",
              " ('what', 8422),\n",
              " ('some', 8306),\n",
              " ('no', 8143),\n",
              " ('her', 7947),\n",
              " ('even', 7687),\n",
              " ('can', 7653),\n",
              " ('has', 7604),\n",
              " ('good', 7423),\n",
              " ('bad', 7401),\n",
              " ('would', 7036),\n",
              " ('up', 6970),\n",
              " ('only', 6781),\n",
              " ('more', 6730),\n",
              " ('when', 6726),\n",
              " ('she', 6444),\n",
              " ('really', 6262),\n",
              " ('time', 6209),\n",
              " ('had', 6142),\n",
              " ('my', 6015),\n",
              " ('were', 6001),\n",
              " ('which', 5780),\n",
              " ('very', 5764),\n",
              " ('me', 5606),\n",
              " ('see', 5452),\n",
              " ('don', 5336),\n",
              " ('we', 5328),\n",
              " ('their', 5278),\n",
              " ('do', 5236),\n",
              " ('story', 5208),\n",
              " ('than', 5183),\n",
              " ('been', 5100),\n",
              " ('much', 5078),\n",
              " ('get', 5037),\n",
              " ('because', 4966),\n",
              " ('people', 4806),\n",
              " ('then', 4761),\n",
              " ('make', 4722),\n",
              " ('how', 4688),\n",
              " ('could', 4686),\n",
              " ('any', 4658),\n",
              " ('into', 4567),\n",
              " ('made', 4541),\n",
              " ('first', 4306),\n",
              " ('other', 4305),\n",
              " ('well', 4254),\n",
              " ('too', 4174),\n",
              " ('them', 4165),\n",
              " ('plot', 4154),\n",
              " ('movies', 4080),\n",
              " ('acting', 4056),\n",
              " ('will', 3993),\n",
              " ('way', 3989),\n",
              " ('most', 3919),\n",
              " ('him', 3858),\n",
              " ('after', 3838),\n",
              " ('its', 3655),\n",
              " ('think', 3643),\n",
              " ('also', 3608),\n",
              " ('characters', 3600),\n",
              " ('off', 3567),\n",
              " ('watch', 3550),\n",
              " ('character', 3506),\n",
              " ('did', 3506),\n",
              " ('why', 3463),\n",
              " ('being', 3393),\n",
              " ('better', 3358),\n",
              " ('know', 3334),\n",
              " ('over', 3316),\n",
              " ('seen', 3265),\n",
              " ('ever', 3263),\n",
              " ('never', 3259),\n",
              " ('your', 3233),\n",
              " ('where', 3219),\n",
              " ('two', 3173),\n",
              " ('little', 3096),\n",
              " ('films', 3077),\n",
              " ('here', 3027),\n",
              " ('m', 3000),\n",
              " ('nothing', 2990),\n",
              " ('say', 2982),\n",
              " ('end', 2954),\n",
              " ('something', 2942),\n",
              " ('should', 2920),\n",
              " ('many', 2909),\n",
              " ('does', 2871),\n",
              " ('thing', 2866),\n",
              " ('show', 2862),\n",
              " ('ve', 2829),\n",
              " ('scene', 2816),\n",
              " ('scenes', 2785),\n",
              " ('these', 2724),\n",
              " ('go', 2717),\n",
              " ('didn', 2646),\n",
              " ('great', 2640),\n",
              " ('watching', 2640),\n",
              " ('re', 2620),\n",
              " ('doesn', 2601),\n",
              " ('through', 2560),\n",
              " ('such', 2544),\n",
              " ('man', 2516),\n",
              " ('worst', 2480),\n",
              " ('actually', 2449),\n",
              " ('actors', 2437),\n",
              " ('life', 2429),\n",
              " ('back', 2424),\n",
              " ('while', 2418),\n",
              " ('director', 2405),\n",
              " ('funny', 2336),\n",
              " ('going', 2319),\n",
              " ('still', 2283),\n",
              " ('another', 2254),\n",
              " ('look', 2247),\n",
              " ('now', 2237),\n",
              " ('old', 2215),\n",
              " ('those', 2212),\n",
              " ('real', 2170),\n",
              " ('few', 2158),\n",
              " ('love', 2152),\n",
              " ('horror', 2150),\n",
              " ('before', 2147),\n",
              " ('want', 2141),\n",
              " ('minutes', 2126),\n",
              " ('pretty', 2115),\n",
              " ('best', 2094),\n",
              " ('though', 2091),\n",
              " ('same', 2081),\n",
              " ('script', 2074),\n",
              " ('work', 2027),\n",
              " ('every', 2025),\n",
              " ('seems', 2023),\n",
              " ('least', 2011),\n",
              " ('enough', 1997),\n",
              " ('down', 1988),\n",
              " ('original', 1983),\n",
              " ('guy', 1964),\n",
              " ('got', 1952),\n",
              " ('around', 1943),\n",
              " ('part', 1942),\n",
              " ('lot', 1892),\n",
              " ('anything', 1874),\n",
              " ('find', 1860),\n",
              " ('new', 1854),\n",
              " ('again', 1849),\n",
              " ('isn', 1849),\n",
              " ('point', 1845),\n",
              " ('things', 1839),\n",
              " ('fact', 1839),\n",
              " ('give', 1823),\n",
              " ('makes', 1814),\n",
              " ('take', 1800),\n",
              " ('thought', 1798),\n",
              " ('d', 1770),\n",
              " ('whole', 1768),\n",
              " ('long', 1761),\n",
              " ('years', 1759),\n",
              " ('however', 1740),\n",
              " ('gets', 1714),\n",
              " ('making', 1695),\n",
              " ('cast', 1694),\n",
              " ('big', 1662),\n",
              " ('might', 1658),\n",
              " ('interesting', 1648),\n",
              " ('money', 1638),\n",
              " ('us', 1628),\n",
              " ('right', 1625),\n",
              " ('far', 1619),\n",
              " ('quite', 1596),\n",
              " ('without', 1595),\n",
              " ('come', 1595),\n",
              " ('almost', 1574),\n",
              " ('ll', 1567),\n",
              " ('action', 1566),\n",
              " ('awful', 1557),\n",
              " ('kind', 1539),\n",
              " ('reason', 1534),\n",
              " ('am', 1530),\n",
              " ('looks', 1528),\n",
              " ('must', 1522),\n",
              " ('done', 1510),\n",
              " ('comedy', 1504),\n",
              " ('someone', 1490),\n",
              " ('trying', 1486),\n",
              " ('wasn', 1484),\n",
              " ('poor', 1481),\n",
              " ('boring', 1478),\n",
              " ('instead', 1478),\n",
              " ('saw', 1475),\n",
              " ('away', 1469),\n",
              " ('girl', 1463),\n",
              " ('probably', 1444),\n",
              " ('believe', 1434),\n",
              " ('sure', 1433),\n",
              " ('looking', 1430),\n",
              " ('stupid', 1428),\n",
              " ('anyone', 1418),\n",
              " ('times', 1406),\n",
              " ('maybe', 1404),\n",
              " ('world', 1404),\n",
              " ('rather', 1394),\n",
              " ('terrible', 1391),\n",
              " ('may', 1390),\n",
              " ('last', 1390),\n",
              " ('since', 1388),\n",
              " ('let', 1385),\n",
              " ('tv', 1382),\n",
              " ('hard', 1374),\n",
              " ('between', 1374),\n",
              " ('waste', 1358),\n",
              " ('woman', 1356),\n",
              " ('feel', 1354),\n",
              " ('effects', 1348),\n",
              " ('half', 1341),\n",
              " ('own', 1333),\n",
              " ('young', 1317),\n",
              " ('music', 1316),\n",
              " ('idea', 1312),\n",
              " ('sense', 1306),\n",
              " ('bit', 1298),\n",
              " ('having', 1280),\n",
              " ('book', 1278),\n",
              " ('found', 1267),\n",
              " ('put', 1263),\n",
              " ('series', 1263),\n",
              " ('goes', 1256),\n",
              " ('worse', 1249),\n",
              " ('said', 1230),\n",
              " ('comes', 1224),\n",
              " ('role', 1222),\n",
              " ('main', 1220),\n",
              " ('else', 1199),\n",
              " ('everything', 1197),\n",
              " ('yet', 1196),\n",
              " ('low', 1189),\n",
              " ('screen', 1188),\n",
              " ('supposed', 1186),\n",
              " ('actor', 1185),\n",
              " ('either', 1183),\n",
              " ('budget', 1179),\n",
              " ('ending', 1179),\n",
              " ('audience', 1178),\n",
              " ('set', 1177),\n",
              " ('family', 1170),\n",
              " ('left', 1169),\n",
              " ('completely', 1168),\n",
              " ('both', 1158),\n",
              " ('wrong', 1155),\n",
              " ('always', 1151),\n",
              " ('course', 1148),\n",
              " ('place', 1148),\n",
              " ('seem', 1147),\n",
              " ('watched', 1142),\n",
              " ('day', 1132),\n",
              " ('simply', 1130),\n",
              " ('shot', 1126),\n",
              " ('mean', 1117),\n",
              " ('special', 1102),\n",
              " ('dead', 1101),\n",
              " ('three', 1094),\n",
              " ('house', 1085),\n",
              " ('oh', 1084),\n",
              " ('night', 1083),\n",
              " ('read', 1082),\n",
              " ('less', 1067),\n",
              " ('high', 1066),\n",
              " ('year', 1064),\n",
              " ('camera', 1061),\n",
              " ('worth', 1057),\n",
              " ('our', 1056),\n",
              " ('try', 1051),\n",
              " ('horrible', 1046),\n",
              " ('sex', 1046),\n",
              " ('video', 1043),\n",
              " ('black', 1039),\n",
              " ('although', 1036),\n",
              " ('couldn', 1036),\n",
              " ('once', 1033),\n",
              " ('rest', 1022),\n",
              " ('dvd', 1021),\n",
              " ('line', 1018),\n",
              " ('played', 1017),\n",
              " ('fun', 1007),\n",
              " ('during', 1006),\n",
              " ('production', 1003),\n",
              " ('everyone', 1002),\n",
              " ('play', 993),\n",
              " ('mind', 990),\n",
              " ('version', 989),\n",
              " ('kids', 989),\n",
              " ('seeing', 988),\n",
              " ('american', 980),\n",
              " ('given', 978),\n",
              " ('used', 969),\n",
              " ('performance', 968),\n",
              " ('especially', 963),\n",
              " ('together', 963),\n",
              " ('tell', 959),\n",
              " ('women', 958),\n",
              " ('start', 956),\n",
              " ('need', 955),\n",
              " ('second', 953),\n",
              " ('takes', 950),\n",
              " ('each', 950),\n",
              " ('wife', 944),\n",
              " ('dialogue', 942),\n",
              " ('use', 940),\n",
              " ('problem', 938),\n",
              " ('star', 934),\n",
              " ('unfortunately', 931),\n",
              " ('himself', 929),\n",
              " ('doing', 926),\n",
              " ('death', 922),\n",
              " ('name', 921),\n",
              " ('lines', 919),\n",
              " ('killer', 914),\n",
              " ('getting', 913),\n",
              " ('help', 905),\n",
              " ('couple', 902),\n",
              " ('fan', 902),\n",
              " ('head', 898),\n",
              " ('crap', 895),\n",
              " ('guess', 888),\n",
              " ('piece', 884),\n",
              " ('nice', 880),\n",
              " ('different', 878),\n",
              " ('school', 876),\n",
              " ('later', 875),\n",
              " ('entire', 869),\n",
              " ('shows', 860),\n",
              " ('next', 858),\n",
              " ('john', 858),\n",
              " ('short', 857),\n",
              " ('seemed', 857),\n",
              " ('hollywood', 850),\n",
              " ('home', 848),\n",
              " ('true', 846),\n",
              " ('person', 846),\n",
              " ('absolutely', 842),\n",
              " ('sort', 840),\n",
              " ('care', 839),\n",
              " ('understand', 836),\n",
              " ('plays', 835),\n",
              " ('felt', 834),\n",
              " ('written', 829),\n",
              " ('title', 828),\n",
              " ('men', 822),\n",
              " ('until', 821),\n",
              " ('flick', 816),\n",
              " ('decent', 815),\n",
              " ('face', 814),\n",
              " ('friends', 810),\n",
              " ('stars', 807),\n",
              " ('job', 807),\n",
              " ('case', 807),\n",
              " ('itself', 804),\n",
              " ('yes', 801),\n",
              " ('perhaps', 800),\n",
              " ('went', 797),\n",
              " ('wanted', 797),\n",
              " ('called', 796),\n",
              " ('annoying', 795),\n",
              " ('ridiculous', 790),\n",
              " ('tries', 790),\n",
              " ('laugh', 788),\n",
              " ('evil', 787),\n",
              " ('along', 786),\n",
              " ('top', 785),\n",
              " ('hour', 784),\n",
              " ('full', 783),\n",
              " ('came', 780),\n",
              " ('writing', 780),\n",
              " ('keep', 770),\n",
              " ('totally', 767),\n",
              " ('playing', 766),\n",
              " ('god', 765),\n",
              " ('won', 764),\n",
              " ('guys', 763),\n",
              " ('already', 762),\n",
              " ('gore', 757),\n",
              " ('direction', 748),\n",
              " ('save', 746),\n",
              " ('lost', 745),\n",
              " ('example', 744),\n",
              " ('sound', 742),\n",
              " ('war', 741),\n",
              " ('attempt', 735),\n",
              " ('car', 733),\n",
              " ('except', 733),\n",
              " ('moments', 732),\n",
              " ('blood', 732),\n",
              " ('obviously', 730),\n",
              " ('act', 729),\n",
              " ('remember', 728),\n",
              " ('kill', 727),\n",
              " ('truly', 726),\n",
              " ('white', 726),\n",
              " ('father', 726),\n",
              " ('b', 725),\n",
              " ('thinking', 720),\n",
              " ('ok', 716),\n",
              " ('finally', 716),\n",
              " ('turn', 711),\n",
              " ('quality', 701),\n",
              " ('lack', 698),\n",
              " ('style', 694),\n",
              " ('wouldn', 693),\n",
              " ('cheap', 691),\n",
              " ('none', 690),\n",
              " ('kid', 686),\n",
              " ('please', 686),\n",
              " ('boy', 685),\n",
              " ('seriously', 684),\n",
              " ('lead', 680),\n",
              " ('dull', 677),\n",
              " ('children', 676),\n",
              " ('starts', 675),\n",
              " ('stuff', 673),\n",
              " ('hope', 672),\n",
              " ('looked', 670),\n",
              " ('recommend', 669),\n",
              " ('under', 668),\n",
              " ('run', 667),\n",
              " ('killed', 667),\n",
              " ('enjoy', 666),\n",
              " ('others', 666),\n",
              " ('etc', 663),\n",
              " ('myself', 663),\n",
              " ('beginning', 662),\n",
              " ('girls', 662),\n",
              " ('against', 662),\n",
              " ('obvious', 660),\n",
              " ('small', 660),\n",
              " ('hell', 659),\n",
              " ('slow', 657),\n",
              " ('hand', 656),\n",
              " ('wonder', 652),\n",
              " ('lame', 652),\n",
              " ('becomes', 651),\n",
              " ('picture', 651),\n",
              " ('based', 650),\n",
              " ('early', 648),\n",
              " ('behind', 646),\n",
              " ('poorly', 644),\n",
              " ('avoid', 642),\n",
              " ('apparently', 640),\n",
              " ('complete', 640),\n",
              " ('happens', 639),\n",
              " ('anyway', 638),\n",
              " ('classic', 637),\n",
              " ('several', 636),\n",
              " ('despite', 635),\n",
              " ('certainly', 635),\n",
              " ('episode', 635),\n",
              " ('often', 631),\n",
              " ('cut', 630),\n",
              " ('writer', 630),\n",
              " ('mother', 628),\n",
              " ('predictable', 628),\n",
              " ('gave', 628),\n",
              " ('become', 627),\n",
              " ('close', 625),\n",
              " ('fans', 624),\n",
              " ('saying', 621),\n",
              " ('scary', 619),\n",
              " ('stop', 618),\n",
              " ('live', 618),\n",
              " ('wants', 617),\n",
              " ('self', 615),\n",
              " ('mr', 612),\n",
              " ('jokes', 611),\n",
              " ('friend', 611),\n",
              " ('cannot', 610),\n",
              " ('overall', 609),\n",
              " ('cinema', 604),\n",
              " ('child', 603),\n",
              " ('silly', 601),\n",
              " ('beautiful', 596),\n",
              " ('human', 595),\n",
              " ('expect', 594),\n",
              " ('liked', 593),\n",
              " ('happened', 592),\n",
              " ('bunch', 590),\n",
              " ('entertaining', 590),\n",
              " ('actress', 588),\n",
              " ('final', 588),\n",
              " ('says', 584),\n",
              " ('performances', 584),\n",
              " ('turns', 577),\n",
              " ('humor', 577),\n",
              " ('themselves', 576),\n",
              " ('eyes', 576),\n",
              " ('hours', 574),\n",
              " ('happen', 573),\n",
              " ('basically', 572),\n",
              " ('days', 572),\n",
              " ('running', 571),\n",
              " ('involved', 569),\n",
              " ('disappointed', 569),\n",
              " ('call', 569),\n",
              " ('directed', 568),\n",
              " ('group', 568),\n",
              " ('fight', 567),\n",
              " ('daughter', 566),\n",
              " ('talking', 566),\n",
              " ('body', 566),\n",
              " ('badly', 565),\n",
              " ('sorry', 565),\n",
              " ('throughout', 563),\n",
              " ('viewer', 563),\n",
              " ('yourself', 562),\n",
              " ('extremely', 562),\n",
              " ('interest', 561),\n",
              " ('heard', 561),\n",
              " ('violence', 561),\n",
              " ('shots', 559),\n",
              " ('side', 557),\n",
              " ('word', 556),\n",
              " ('art', 555),\n",
              " ('possible', 554),\n",
              " ('dark', 551),\n",
              " ('game', 551),\n",
              " ('hero', 550),\n",
              " ('alone', 549),\n",
              " ('son', 547),\n",
              " ('type', 547),\n",
              " ('leave', 547),\n",
              " ('gives', 546),\n",
              " ('parts', 546),\n",
              " ('single', 546),\n",
              " ('started', 545),\n",
              " ('female', 543),\n",
              " ('rating', 541),\n",
              " ('mess', 541),\n",
              " ('voice', 541),\n",
              " ('aren', 540),\n",
              " ('town', 540),\n",
              " ('drama', 538),\n",
              " ('definitely', 537),\n",
              " ('unless', 536),\n",
              " ('review', 534),\n",
              " ('effort', 533),\n",
              " ('weak', 533),\n",
              " ('able', 533),\n",
              " ('took', 531),\n",
              " ('non', 530),\n",
              " ('five', 530),\n",
              " ('matter', 529),\n",
              " ('usually', 529),\n",
              " ('michael', 528),\n",
              " ('feeling', 526),\n",
              " ('huge', 523),\n",
              " ('sequel', 522),\n",
              " ('soon', 521),\n",
              " ('exactly', 520),\n",
              " ('past', 519),\n",
              " ('turned', 518),\n",
              " ('police', 518),\n",
              " ('tried', 515),\n",
              " ('middle', 513),\n",
              " ('talent', 513),\n",
              " ('genre', 512),\n",
              " ('zombie', 510),\n",
              " ('ends', 509),\n",
              " ('history', 509),\n",
              " ('straight', 503),\n",
              " ('opening', 501),\n",
              " ('serious', 501),\n",
              " ('coming', 501),\n",
              " ('moment', 500),\n",
              " ('lives', 499),\n",
              " ('sad', 499),\n",
              " ('dialog', 498),\n",
              " ('particularly', 498),\n",
              " ('editing', 493),\n",
              " ('clearly', 492),\n",
              " ('beyond', 491),\n",
              " ('earth', 491),\n",
              " ('taken', 490),\n",
              " ('cool', 490),\n",
              " ('level', 489),\n",
              " ('dumb', 489),\n",
              " ('okay', 488),\n",
              " ('major', 487),\n",
              " ('fast', 485),\n",
              " ('premise', 485),\n",
              " ('joke', 484),\n",
              " ('stories', 484),\n",
              " ('wasted', 483),\n",
              " ('minute', 483),\n",
              " ('across', 482),\n",
              " ('mostly', 482),\n",
              " ('rent', 482),\n",
              " ('late', 481),\n",
              " ('falls', 481),\n",
              " ('fails', 481),\n",
              " ('mention', 478),\n",
              " ('theater', 475),\n",
              " ('stay', 472),\n",
              " ('sometimes', 472),\n",
              " ('hit', 468),\n",
              " ('talk', 467),\n",
              " ('fine', 467),\n",
              " ('die', 466),\n",
              " ('storyline', 465),\n",
              " ('pointless', 465),\n",
              " ('taking', 464),\n",
              " ('order', 462),\n",
              " ('brother', 461),\n",
              " ('whatever', 460),\n",
              " ('told', 460),\n",
              " ('wish', 458),\n",
              " ('room', 456),\n",
              " ('career', 455),\n",
              " ('appears', 455),\n",
              " ('write', 455),\n",
              " ('known', 454),\n",
              " ('husband', 454),\n",
              " ('living', 451),\n",
              " ('sit', 450),\n",
              " ('ten', 450),\n",
              " ('words', 449),\n",
              " ('monster', 448),\n",
              " ('chance', 448),\n",
              " ('hate', 444),\n",
              " ('novel', 444),\n",
              " ('add', 443),\n",
              " ('english', 443),\n",
              " ('somehow', 441),\n",
              " ('strange', 440),\n",
              " ('imdb', 438),\n",
              " ('actual', 438),\n",
              " ('total', 437),\n",
              " ('material', 437),\n",
              " ('killing', 437),\n",
              " ('ones', 437),\n",
              " ('knew', 436),\n",
              " ('king', 434),\n",
              " ('number', 434),\n",
              " ('using', 433),\n",
              " ('lee', 431),\n",
              " ('power', 431),\n",
              " ('shown', 431),\n",
              " ('works', 431),\n",
              " ('giving', 431),\n",
              " ('points', 430),\n",
              " ('possibly', 430),\n",
              " ('kept', 430),\n",
              " ('four', 429),\n",
              " ('local', 427),\n",
              " ('usual', 426),\n",
              " ('including', 425),\n",
              " ('problems', 424),\n",
              " ('ago', 424),\n",
              " ('opinion', 424),\n",
              " ('nudity', 423),\n",
              " ('age', 422),\n",
              " ('due', 421),\n",
              " ('roles', 420),\n",
              " ('writers', 419),\n",
              " ('decided', 419),\n",
              " ('near', 418),\n",
              " ('flat', 418),\n",
              " ('easily', 418),\n",
              " ('murder', 417),\n",
              " ('experience', 417),\n",
              " ('reviews', 416),\n",
              " ('imagine', 415),\n",
              " ('feels', 413),\n",
              " ('plain', 411),\n",
              " ('somewhat', 411),\n",
              " ('class', 410),\n",
              " ('score', 410),\n",
              " ('song', 409),\n",
              " ('bring', 409),\n",
              " ('whether', 409),\n",
              " ('otherwise', 408),\n",
              " ('whose', 408),\n",
              " ('average', 408),\n",
              " ('pathetic', 407),\n",
              " ('nearly', 407),\n",
              " ('knows', 407),\n",
              " ('zombies', 407),\n",
              " ('cinematography', 406),\n",
              " ('cheesy', 406),\n",
              " ('upon', 406),\n",
              " ('city', 405),\n",
              " ('space', 405),\n",
              " ('credits', 404),\n",
              " ('james', 403),\n",
              " ('lots', 403),\n",
              " ('change', 403),\n",
              " ('entertainment', 402),\n",
              " ('nor', 402),\n",
              " ('wait', 401),\n",
              " ('released', 400),\n",
              " ('needs', 399),\n",
              " ('shame', 398),\n",
              " ('attention', 396),\n",
              " ('comments', 394),\n",
              " ('bored', 393),\n",
              " ('free', 393),\n",
              " ('lady', 393),\n",
              " ('expected', 392),\n",
              " ('needed', 392),\n",
              " ('clear', 392),\n",
              " ('view', 391),\n",
              " ('development', 390),\n",
              " ('check', 390),\n",
              " ('doubt', 390),\n",
              " ('figure', 389),\n",
              " ('mystery', 389),\n",
              " ('excellent', 388),\n",
              " ('garbage', 388),\n",
              " ('sequence', 386),\n",
              " ('television', 386),\n",
              " ('o', 385),\n",
              " ('sets', 385),\n",
              " ('laughable', 384),\n",
              " ('potential', 384),\n",
              " ('robert', 382),\n",
              " ('light', 382),\n",
              " ('country', 382),\n",
              " ('documentary', 382),\n",
              " ('reality', 382),\n",
              " ('general', 381),\n",
              " ('ask', 381),\n",
              " ('comic', 380),\n",
              " ('fall', 380),\n",
              " ('begin', 380),\n",
              " ('footage', 379),\n",
              " ('stand', 379),\n",
              " ('forced', 379),\n",
              " ('trash', 379),\n",
              " ('remake', 379),\n",
              " ('thriller', 378),\n",
              " ('songs', 378),\n",
              " ('gay', 377),\n",
              " ('within', 377),\n",
              " ('hardly', 376),\n",
              " ('above', 375),\n",
              " ('gone', 375),\n",
              " ('george', 374),\n",
              " ('means', 373),\n",
              " ('sounds', 373),\n",
              " ('directing', 372),\n",
              " ('move', 372),\n",
              " ('david', 372),\n",
              " ('buy', 372),\n",
              " ('rock', 371),\n",
              " ('forward', 371),\n",
              " ('important', 371),\n",
              " ('hot', 370),\n",
              " ('haven', 370),\n",
              " ('filmed', 370),\n",
              " ('british', 370),\n",
              " ('heart', 369),\n",
              " ('reading', 369),\n",
              " ('fake', 369),\n",
              " ('incredibly', 368),\n",
              " ('weird', 368),\n",
              " ('hear', 368),\n",
              " ('enjoyed', 367),\n",
              " ('hilarious', 367),\n",
              " ('cop', 367),\n",
              " ('musical', 367),\n",
              " ('message', 366),\n",
              " ('happy', 366),\n",
              " ('pay', 366),\n",
              " ('laughs', 365),\n",
              " ('box', 365),\n",
              " ('suspense', 363),\n",
              " ('sadly', 363),\n",
              " ('eye', 362),\n",
              " ('third', 361),\n",
              " ('similar', 361),\n",
              " ('named', 361),\n",
              " ('modern', 360),\n",
              " ('failed', 359),\n",
              " ('events', 359),\n",
              " ('forget', 358),\n",
              " ('question', 358),\n",
              " ('male', 357),\n",
              " ('finds', 357),\n",
              " ('perfect', 356),\n",
              " ('spent', 355),\n",
              " ('sister', 355),\n",
              " ('feature', 354),\n",
              " ('result', 354),\n",
              " ('comment', 353),\n",
              " ('girlfriend', 353),\n",
              " ('sexual', 352),\n",
              " ('attempts', 351),\n",
              " ('neither', 351),\n",
              " ('richard', 351),\n",
              " ('screenplay', 350),\n",
              " ('elements', 350),\n",
              " ('spoilers', 349),\n",
              " ('brain', 348),\n",
              " ('filmmakers', 348),\n",
              " ('showing', 348),\n",
              " ('miss', 347),\n",
              " ('dr', 347),\n",
              " ('christmas', 347),\n",
              " ('cover', 345),\n",
              " ('red', 344),\n",
              " ('sequences', 344),\n",
              " ('typical', 343),\n",
              " ('excuse', 343),\n",
              " ('crazy', 342),\n",
              " ('ideas', 342),\n",
              " ('baby', 342),\n",
              " ('loved', 341),\n",
              " ('meant', 341),\n",
              " ('worked', 340),\n",
              " ('fire', 340),\n",
              " ('unbelievable', 339),\n",
              " ('follow', 339),\n",
              " ('theme', 337),\n",
              " ('barely', 336),\n",
              " ('producers', 336),\n",
              " ('twist', 336),\n",
              " ('plus', 336),\n",
              " ('appear', 336),\n",
              " ('directors', 335),\n",
              " ('team', 335),\n",
              " ('viewers', 333),\n",
              " ('leads', 332),\n",
              " ('tom', 332),\n",
              " ('slasher', 332),\n",
              " ('wrote', 331),\n",
              " ('villain', 331),\n",
              " ('gun', 331),\n",
              " ('working', 331),\n",
              " ('island', 330),\n",
              " ('strong', 330),\n",
              " ('open', 330),\n",
              " ('realize', 330),\n",
              " ('positive', 329),\n",
              " ('disappointing', 329),\n",
              " ('yeah', 329),\n",
              " ('quickly', 329),\n",
              " ('weren', 328),\n",
              " ('release', 328),\n",
              " ('simple', 328),\n",
              " ('honestly', 328),\n",
              " ('eventually', 327),\n",
              " ('period', 327),\n",
              " ('tells', 327),\n",
              " ('kills', 327),\n",
              " ('doctor', 327),\n",
              " ('nowhere', 326),\n",
              " ('list', 326),\n",
              " ('acted', 326),\n",
              " ('herself', 326),\n",
              " ('dog', 326),\n",
              " ('walk', 325),\n",
              " ('air', 324),\n",
              " ('apart', 324),\n",
              " ('makers', 323),\n",
              " ('subject', 323),\n",
              " ('learn', 322),\n",
              " ('fi', 322),\n",
              " ('sci', 319),\n",
              " ('bother', 319),\n",
              " ('admit', 319),\n",
              " ('jack', 318),\n",
              " ('disappointment', 318),\n",
              " ('hands', 318),\n",
              " ('note', 318),\n",
              " ('certain', 317),\n",
              " ('e', 317),\n",
              " ('value', 317),\n",
              " ('casting', 317),\n",
              " ('grade', 316),\n",
              " ('peter', 316),\n",
              " ('suddenly', 315),\n",
              " ('missing', 315),\n",
              " ('form', 313),\n",
              " ('stick', 313),\n",
              " ('previous', 313),\n",
              " ('break', 313),\n",
              " ('soundtrack', 312),\n",
              " ('surprised', 311),\n",
              " ('front', 311),\n",
              " ('expecting', 311),\n",
              " ('parents', 310),\n",
              " ('surprise', 310),\n",
              " ('relationship', 310),\n",
              " ('shoot', 309),\n",
              " ('today', 309),\n",
              " ('painful', 308),\n",
              " ('ways', 308),\n",
              " ('leaves', 308),\n",
              " ('ended', 308),\n",
              " ('creepy', 308),\n",
              " ('concept', 308),\n",
              " ('somewhere', 308),\n",
              " ('vampire', 308),\n",
              " ('spend', 307),\n",
              " ('th', 307),\n",
              " ('future', 306),\n",
              " ('difficult', 306),\n",
              " ('effect', 306),\n",
              " ('fighting', 306),\n",
              " ('street', 306),\n",
              " ('c', 305),\n",
              " ('america', 305),\n",
              " ('accent', 304),\n",
              " ('truth', 302),\n",
              " ('project', 302),\n",
              " ('joe', 301),\n",
              " ('f', 301),\n",
              " ('deal', 301),\n",
              " ('indeed', 301),\n",
              " ('biggest', 300),\n",
              " ('rate', 300),\n",
              " ('paul', 299),\n",
              " ('japanese', 299),\n",
              " ('utterly', 298),\n",
              " ('begins', 298),\n",
              " ('redeeming', 298),\n",
              " ('college', 298),\n",
              " ('york', 297),\n",
              " ('fairly', 297),\n",
              " ('disney', 297),\n",
              " ('crew', 296),\n",
              " ('create', 296),\n",
              " ('cartoon', 296),\n",
              " ('revenge', 296),\n",
              " ('co', 295),\n",
              " ('outside', 295),\n",
              " ('computer', 295),\n",
              " ('interested', 295),\n",
              " ('stage', 295),\n",
              " ('considering', 294),\n",
              " ('speak', 294),\n",
              " ('among', 294),\n",
              " ('towards', 293),\n",
              " ('channel', 293),\n",
              " ('sick', 293),\n",
              " ('talented', 292),\n",
              " ('cause', 292),\n",
              " ('particular', 292),\n",
              " ('van', 292),\n",
              " ('hair', 292),\n",
              " ('bottom', 291),\n",
              " ('reasons', 291),\n",
              " ('mediocre', 290),\n",
              " ('cat', 290),\n",
              " ('telling', 290),\n",
              " ('supporting', 289),\n",
              " ('store', 289),\n",
              " ('hoping', 288),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hgH0uO-SBGS",
        "colab_type": "text"
      },
      "source": [
        "The above is just to show the most common words in the positive and negative sentences. However, there are a lot of unnecessary words like `the`, `a`, `was`, and so on. Can you find a way to show the relevant words and not these words? \n",
        "\n",
        "```\n",
        "Hint: Stop Words removal or normalizing each term.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-0YttJsSBGT",
        "colab_type": "code",
        "outputId": "452cbac4-795d-4e9a-b602-981b4e619026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'ran',\n",
              " 'time',\n",
              " 'programs',\n",
              " 'school',\n",
              " 'life',\n",
              " 'teachers',\n",
              " 'years',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'believe',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " 'satire',\n",
              " 'much',\n",
              " 'closer',\n",
              " 'reality',\n",
              " 'teachers',\n",
              " 'scramble',\n",
              " 'survive',\n",
              " 'financially',\n",
              " 'insightful',\n",
              " 'students',\n",
              " 'see',\n",
              " 'right',\n",
              " 'pathetic']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb985nidSBGV",
        "colab_type": "code",
        "outputId": "b73bf2f6-67bf-497d-878c-22117856544b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "[vocab_to_int[word] for word in words[:30]]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8209,\n",
              " 29941,\n",
              " 9817,\n",
              " 12475,\n",
              " 52571,\n",
              " 66112,\n",
              " 51087,\n",
              " 57179,\n",
              " 37763,\n",
              " 64925,\n",
              " 73368,\n",
              " 64927,\n",
              " 51028,\n",
              " 37157,\n",
              " 5655,\n",
              " 8209,\n",
              " 29941,\n",
              " 56729,\n",
              " 43355,\n",
              " 11907,\n",
              " 52975,\n",
              " 64925,\n",
              " 57371,\n",
              " 63875,\n",
              " 23636,\n",
              " 32888,\n",
              " 62960,\n",
              " 57734,\n",
              " 54984,\n",
              " 47815]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kha9XzkSBGX",
        "colab_type": "code",
        "outputId": "e93cdc07-3ec7-4567-a591-487e7d791a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_to_int['bromwell']"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8209"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOiwo7U1SBGZ",
        "colab_type": "text"
      },
      "source": [
        "## One hot encoding\n",
        "\n",
        "We need one hot encoding for the labels. Think of a reason why we need one hot encoded labels for classes?\n",
        "\n",
        "## Task 3: Create one hot encoding for the labels. \n",
        "\n",
        "* Write the one hot encoding logic in the `one_hot` function.\n",
        "* Use 1 for positive label and 0 for negative label.\n",
        "* Save all the values in the `encoded_labels` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiB21eZ9SBGa",
        "colab_type": "code",
        "outputId": "6c2e6270-d960-4034-9212-afa2efa266cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1 for positive label and 0 for negative label\n",
        "def one_hot(labels):\n",
        "    return list(map(lambda x: 1 if x == 'positive\\n' else 0, labels))\n",
        "\n",
        "encoded_labels = one_hot(labels)\n",
        "print(len(encoded_labels))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8tour2VSBGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print the length of your label and uncomment next line only if the encoded_labels size is 25001.\n",
        "# If you dont get the intuition behind this step, print encoded_labels to see it.\n",
        "# encoded_labels = encoded_labels[:25000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwjtjkTjSBGf",
        "colab_type": "code",
        "outputId": "a1909762-7342-4750-d6b5-b06e25a9b774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(encoded_labels)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1FFvHSfSBGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "    try:\n",
        "        reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
        "    except KeyError: # this can happen because we removed stopwords, ignore this\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnMq7NOXSBGj",
        "colab_type": "code",
        "outputId": "75b0ccda-5fb6-4cfe-8471-5dd758d8475a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# This step is to see if any review is empty and we remove it. Otherwise the input will be all zeroes.\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 1443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gd2ksAcSBGl",
        "colab_type": "code",
        "outputId": "8387c6eb-5119-4317-ce8d-c0c333bf0e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "\n",
        "# get indices of any reviews with length 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "# remove 0-length reviews and their labels\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  23561\n",
            "Number of reviews after removing outliers:  23560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3SQ_FJ3SBGo",
        "colab_type": "code",
        "outputId": "575329eb-70da-4e06-b75d-0696e10cb946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(encoded_labels)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NuXiPVeSBGq",
        "colab_type": "text"
      },
      "source": [
        "## Task 4: Padding the data\n",
        "\n",
        "> Define a function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network. \n",
        "* The data should come from `review_ints`, since we want to feed integers to the network. \n",
        "* Each row should be `seq_length` elements long. \n",
        "* For reviews shorter than `seq_length` words, **left pad** with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. \n",
        "* For reviews longer than `seq_length`, use only the first `seq_length` words as the feature vector.\n",
        "\n",
        "As a small example, if the `seq_length=10` and an input review is: \n",
        "```\n",
        "[117, 18, 128]\n",
        "```\n",
        "The resultant, padded sequence should be: \n",
        "\n",
        "```\n",
        "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
        "```\n",
        "\n",
        "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCA0K9mbSBGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Write the logic for padding the data\n",
        "def pad_features(reviews_ints, seq_length):\n",
        "    features = []\n",
        "    for review in reviews_ints:\n",
        "        pad = seq_length - len(review)\n",
        "        if pad > 0:\n",
        "            for _ in range(pad):\n",
        "                review.insert(0, 0)\n",
        "\n",
        "        features.append(review[:seq_length])\n",
        "\n",
        "    return np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsX6jr7DSBGu",
        "colab_type": "code",
        "outputId": "50dea6f1-96c5-4f4c-f6ba-a9f46eca3061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Verify if everything till now is correct. \n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [30457 30883 26251  9668 62110 33745 73368 44531 49359 29586]\n",
            " [ 1288 62095  7767 44542 39000 49363 38204 69926 47190  5708]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [71963 38129 47558 66519  1697 65525 57734 33167 46047 39907]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [32070 14606   341 51517 65860  5053 67560 21733 25538 55274]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [65521 44744 38074 55274 72322 66495 12324  7019  9390 55582]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81F8J6kcSBGw",
        "colab_type": "text"
      },
      "source": [
        "Now we have everything ready. It's time to split our dataset into `Train`, `Test` and `Validate`. \n",
        "\n",
        "Read more about the train-test-split here : https://cs230-stanford.github.io/train-dev-test-split.html\n",
        "\n",
        "## Task 5: Lets create train, test and val split in the ratio of 8:1:1.  \n",
        "\n",
        "Hint: Either use shuffle and slicing in Python or use train-test-val split in Sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2e7Y-8SBGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_frac = 0.8\n",
        "val_frac = 0.1\n",
        "test_frac = 0.1\n",
        "\n",
        "def train_test_val_split(features):\n",
        "    val_test_x, train_x = train_test_split(features, test_size=train_frac, shuffle=False)\n",
        "    val_x, test_x = train_test_split(val_test_x, test_size=test_frac / (val_frac+test_frac), shuffle=False)\n",
        "    return train_x, val_x, test_x\n",
        "\n",
        "def train_test_val_labels(encoded_labels):\n",
        "    val_test_y, train_y = train_test_split(encoded_labels, test_size=train_frac, shuffle=False)\n",
        "    val_y, test_y = train_test_split(val_test_y, test_size=test_frac / (val_frac+test_frac), shuffle=False)\n",
        "    return train_y, val_y, test_y\n",
        "\n",
        "train_x, val_x, test_x = train_test_val_split(features)\n",
        "train_y, val_y, test_y = train_test_val_labels(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZv-1Es8SBGy",
        "colab_type": "code",
        "outputId": "9806029a-9b68-4d12-8ff8-712dd65c32b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(18848, 200) \n",
            "Validation set: \t(2356, 200) \n",
            "Test set: \t\t(2356, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQnGTKQNSBG0",
        "colab_type": "text"
      },
      "source": [
        "## DataLoaders and Batching\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
        "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
        "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
        "\n",
        "```\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "```\n",
        "\n",
        "This is an alternative to creating a generator function for batching our data into full batches.\n",
        "\n",
        "### Task 6: Create a generator function for the dataset. \n",
        "See the above link for more info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm4hpwtdSBG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets for train, test and val\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "# make sure to SHUFFLE your training data. Keep Shuffle=True.\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Aqq1rbSBG3",
        "colab_type": "code",
        "outputId": "fc059e19-fd43-4569-8f68-1937f8bc7999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# obtain one batch of training data and label. \n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([64, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ..., 63787, 43645, 33568],\n",
            "        [    0,     0,     0,  ..., 24580, 71448, 46047],\n",
            "        [45872, 10380,  6705,  ..., 14656,  3823, 23579],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ..., 71448,  7680,  7680],\n",
            "        [    0,     0,     0,  ..., 71448, 35995, 71351],\n",
            "        [    0,     0,     0,  ..., 14261, 39641, 14189]])\n",
            "\n",
            "Sample label size:  torch.Size([64])\n",
            "Sample label: \n",
            " tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxwbMF7wSBG6",
        "colab_type": "code",
        "outputId": "fea83b89-2778-413d-98c5-6af8fcf31ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check if GPU is available.\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyedLs8HSBG8",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Model \n",
        "\n",
        "Here we are creating a simple RNN in PyTorch and pass the output to the a Linear layer and Sigmoid at the end to get the probability score and prediction as POSITIVE or NEGATIVE. \n",
        "\n",
        "The network is very similar to the CNN network created in Exercise 2. \n",
        "\n",
        "More info available at: https://pytorch.org/docs/0.3.1/nn.html?highlight=rnn#torch.nn.RNN\n",
        "\n",
        "Read about the parameters that the RNN takes and see what will happen when `batch_first` is set as `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifv_qOrnSBG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_dim, n_layers, \n",
        "                          dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # RNN out layer\n",
        "        rnn_out, hidden = self.rnn(x, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        rnn_out = rnn_out.view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(rnn_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjOaZ4ndSBG_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Task 7 : Know the shape\n",
        "\n",
        "Given a batch of 64 and input size as 1 and a sequence length of 200 to a RNN with 2 stacked layers and 512 hidden layers, find the shape of input data (x) and the hidden dimension (hidden) specified in the forward pass of the network. Note, the batch_first is kept to be True. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLAFWod8SBHA",
        "colab_type": "code",
        "outputId": "93a19a09-a286-4a79-e195-1a828b4e83f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (rnn): RNN(74038, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJB_fUSTSBHE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Task 8: LSTM \n",
        "\n",
        "Before we start creating the LSTM, it is important to understand LSTM and to know why we prefer LSTM over a Vanilla RNN for this task. \n",
        "> Here are some good links to know about LSTM:\n",
        "* [Colah Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "* [Understanding LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
        "* [RNN effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "\n",
        "Now create a class named SentimentLSTM with `n_layers=2`, and rest all hyperparameters same as before. Also, create an embedding layer and feed the output of the embedding layer as input to the LSTM model. Dont forget to add a regularizer (dropout) layer after the LSTM layer with p=0.4 to prevent overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAeKXfnCSBHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The LSTM model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.4):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # We need to get word embeddings before passing them to the LSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # The dropout layer regularizes our network\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        n = x.size(0)\n",
        "        x = x.long()\n",
        "\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(n, -1)[:,-1]\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXIEIPdTSBHI",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate the network\n",
        "\n",
        "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
        "\n",
        "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBpD9eoUSBHJ",
        "colab_type": "code",
        "outputId": "b6977cac-8f79-436b-a2e1-f607548e7431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Instantiate the model with these hyperparameters\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 200\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(74038, 200)\n",
            "  (lstm): LSTM(200, 512, num_layers=2, batch_first=True, dropout=0.4)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRFoITeKSBHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azzNrD1dSBHP",
        "colab_type": "text"
      },
      "source": [
        "### Task 9: Loss Functions\n",
        "We are using `BCELoss (Binary Cross Entropy Loss)` since we have two output classes. \n",
        "\n",
        "Can Cross Entropy Loss be used instead of BCELoss? \n",
        "\n",
        "If no, why not? If yes, how?\n",
        "\n",
        "Is `NLLLoss()` and last layer as `LogSoftmax()` is same as using `CrossEntropyLoss()` with a Softmax final layer? Can you get the mathematical intuition behind it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCO1YVGaSBHQ",
        "colab_type": "code",
        "outputId": "1a190901-e125-4905-da24-8b442b4c7922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Training and Validation\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.694461... Val Loss: 0.693437\n",
            "Epoch: 1/4... Step: 200... Loss: 0.703075... Val Loss: 0.699210\n",
            "Epoch: 2/4... Step: 300... Loss: 0.692223... Val Loss: 0.693139\n",
            "Epoch: 2/4... Step: 400... Loss: 0.685716... Val Loss: 0.693205\n",
            "Epoch: 2/4... Step: 500... Loss: 0.692228... Val Loss: 0.693213\n",
            "Epoch: 3/4... Step: 600... Loss: 0.699633... Val Loss: 0.693148\n",
            "Epoch: 3/4... Step: 700... Loss: 0.700249... Val Loss: 0.693158\n",
            "Epoch: 3/4... Step: 800... Loss: 0.694715... Val Loss: 0.693244\n",
            "Epoch: 4/4... Step: 900... Loss: 0.694279... Val Loss: 0.693183\n",
            "Epoch: 4/4... Step: 1000... Loss: 0.695668... Val Loss: 0.693288\n",
            "Epoch: 4/4... Step: 1100... Loss: 0.701219... Val Loss: 0.693280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOftkL6LSBHS",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Once we are done with training and validating, we can improve training loss and validation loss by playing around with the hyperparameters. Can you find a better set of hyperparams? Play around with it. \n",
        "\n",
        "### Task 10: Prediction Function\n",
        "Now write a prediction function to predict the output for the test set created. Save the results in a CSV file with one column as the reviews and the prediction in the next column. Calculate the accuracy of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qByamd14SBHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eda85f90-751c-44c1-98c1-22c5248172e4"
      },
      "source": [
        "import csv\n",
        "\n",
        "def to_sentences(input):\n",
        "    sentences=[]\n",
        "    for batch_instance in range(0, 64):\n",
        "        sentence=\"\"\n",
        "        for id in input[batch_instance]:\n",
        "            if(id==0):\n",
        "                continue\n",
        "\n",
        "            sentence = sentence + str(vocab[id]) + ' '\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return np.array(sentences)\n",
        "\n",
        "\n",
        "def predict():\n",
        "    test_losses = []\n",
        "    num_correct = 0\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    with open('results.csv', 'w') as csvfile:  # file to save results\n",
        "        outfile = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "        outfile.writerow([\"Review\", \"Predicted Sentiment\"])\n",
        "\n",
        "        net.eval()\n",
        "        for inputs, labels in test_loader:\n",
        "            if(train_on_gpu):\n",
        "              inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "            output, h = net(inputs, h)\n",
        "            test_loss = criterion(output.squeeze(), labels.float())\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
        "            correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "            correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "            num_correct += np.sum(correct)\n",
        "\n",
        "            sentences = to_sentences(inputs)\n",
        "            for i in range(0, batch_size):\n",
        "                outfile.writerow([sentences[i], 'Positive' if output[i] == 1 else 'Negative'])\n",
        "\n",
        "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "    test_acc = num_correct/len(test_loader.dataset)\n",
        "    print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n",
        "\n",
        "predict()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.693\n",
            "Test accuracy: 49.448%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIKZUwB5SBHV",
        "colab_type": "text"
      },
      "source": [
        "## Bonus Question: Create an app using Flask\n",
        "\n",
        "> Extra bonus points if someone attempts this question:\n",
        "* Save the trained model checkpoints.\n",
        "* Create a Flask app and load the model. A similar work in the field of CNN has been done here : https://github.com/kumar-shridhar/Business-Card-Detector (Check `app.py`)\n",
        "* You can use hosting services like Heroku and/or with Docker to host your app and show it to everyone. \n",
        "Example here: https://github.com/selimrbd/sentiment_analysis/blob/master/Dockerfile\n"
      ]
    }
  ]
}