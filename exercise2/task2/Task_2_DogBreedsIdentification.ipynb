{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Task_2_DogBreedsIdentification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gruppe3VDL/Gruppe3VDLws2019/blob/master/exercise2/task2/Task_2_DogBreedsIdentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uJd6k4jiGcr",
        "colab_type": "text"
      },
      "source": [
        "__Complete all sub-tasks marked with ## TO DO! ## and submit the filled notebook on OLAT__ \\\n",
        "__Using a GPU is recommended here__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTk2h0lXiGdI",
        "colab_type": "text"
      },
      "source": [
        "### Transfer Learning ###\n",
        "Aim of this notebook is to implement the concept of transfer learning to train a bigger dataset. We try to compete on a well-known competiton on Kaggle known as Dog Breeds Identification. Read more about it here:\n",
        "\n",
        "https://www.kaggle.com/c/dog-breed-identification/overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPlCpQl1iGdT",
        "colab_type": "text"
      },
      "source": [
        "To train a model on the Dog breeds dataset using transfer learning and submit your results to Kaggle.\n",
        "Note: Below notebook gives some tips to run the code in pytorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuiPm2ADiGdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEQs2oPIedzq",
        "colab_type": "code",
        "outputId": "e6b05eb7-b925-4bbf-8bea-bcba2d8a4176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln-9YzNLNQP_",
        "colab_type": "code",
        "outputId": "0ac660b7-9379-407d-a54a-546b828d3164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/Task_2/data'"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Task_2/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLNupTuZNarK",
        "colab_type": "code",
        "outputId": "bcc5e087-f2aa-4bb2-a3ca-b9c55aad8d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls train/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA1uuJSwiGeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Be02QbmiGeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBG9tDSriGfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from AlexNet import AlexNet\n",
        "from train_test import start_train_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpC1eJ-YiGfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## Register on Kaggle With Your repective GroupName  (For example: WS19_VDL_GROUP_01)    ##\n",
        "##                                                                                       ##\n",
        "##                                                                                       ##\n",
        "##                                   ##                                                  ##\n",
        "##                                  #  #  ###                                            ##\n",
        "##                                   ##  #                                               ##\n",
        "##                                 ######                                                ##\n",
        "##                                #  ##                                                  ##\n",
        "##                             ###   ##                                                  ##\n",
        "##                                   ##                                                  ##\n",
        "##                                ###  ###                                               ##\n",
        "##                                                                                       ##\n",
        "##                                                                                       ##\n",
        "###########################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ8VLTQBiGf8",
        "colab_type": "code",
        "outputId": "f135258d-77a5-41ab-df1f-c554c521ad29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "###########################################################################################\n",
        "## Download the Dog-Breeds dataset in folder \"data\"                                      ##\n",
        "## from the Kaggle competition link mentioned above                                      ##\n",
        "##                                                                                       ##\n",
        "##                                                                                       ##\n",
        "##                                   ##                                                  ##\n",
        "##                                  #  #  ###                                            ##\n",
        "##                                   ##  #                                               ##\n",
        "##                                 ######                                                ##\n",
        "##                                #  ##                                                  ##\n",
        "##                             ###   ##                                                  ##\n",
        "##                                   ##                                                  ##\n",
        "##                                ###  ###                                               ##\n",
        "##                                                                                       ##\n",
        "##                                                                                       ##\n",
        "###########################################################################################\n",
        "# !mkdir /root/.kaggle\n",
        "# !echo '{\"username\":\"ws19vdlgroup03\",\"key\":\"820848a6160b4d9eeb38e21cb8ba74c3\"}' > /root/.kaggle/kaggle.json\n",
        "# !pip install kaggle\n",
        "# !kaggle competitions download -c dog-breed-identification"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "labels.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEoxTjhiGgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################################################\n",
        "## Make your dataset to and dataloaders for the  test data                                ##\n",
        "############################################################################################\n",
        "import torchvision\n",
        "\n",
        "#!unzip 'train.zip'\n",
        "#!unzip 'test.zip'\n",
        "#!unzip 'labels.csv.zip'\n",
        "\n",
        "\n",
        "def load_dataset(src):\n",
        "    data_path = src\n",
        "    test_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transforms.Compose([\n",
        "                                      torchvision.transforms.Resize((224, 224)),\n",
        "                                      torchvision.transforms.ToTensor()])\n",
        "    )\n",
        "    test_dataset = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=64,\n",
        "        num_workers=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "    return test_dataset\n",
        "   \n",
        "test_dataset = load_dataset('test/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v91dtTDiGgy",
        "colab_type": "code",
        "outputId": "d97e7c87-e861-4a3e-8437-8e90f3b901b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "####################################################################################################\n",
        "## TO DO! : Split train data into 20% validation set and make dataloaders for train and val split ##\n",
        "####################################################################################################\n",
        "#!mkdir train/train_images\n",
        "#!mv train/*.jpg train/train_images/ \n",
        "\n",
        "#!mkdir test/test_images\n",
        "#!mv test/*.jpg test/test_images/ \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=\"train/\", transform=data_transforms)\n",
        "train_labels = pd.read_csv('labels.csv')\n",
        "\n",
        "y_train, y_validation = train_test_split(train_labels, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Train Set\", y_train.shape);\n",
        "print(\"Validation Set\", y_validation.shape);"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Set (8177, 2)\n",
            "Validation Set (2045, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aY7kJ0MiGhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HINT: \n",
        "# One can make their own custom dataset and dataloaders using the CSV file or\n",
        "# Convert the Dog-breed training dataset into Imagenet Format, where all images of one class are in a\n",
        "# folder named with class as in the below given format. Standard Pytorch Datasets and Dataloaders can then be applied\n",
        "# over them\n",
        "# Root\n",
        "# |\n",
        "# |---Class1 ___Img1.png\n",
        "# |          ___Img2.png\n",
        "# |\n",
        "# |---Class2 ___Img3.png\n",
        "# |          ___Img4.png\n",
        "# |....\n",
        "# |....\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "\n",
        "train_dir = \"converted/train/\"\n",
        "valid_dir = \"converted/valid/\"\n",
        "\n",
        "ids = train_labels['id'].to_numpy()\n",
        "breeds = train_labels['breed'].to_numpy()\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "  os.makedirs(train_dir)\n",
        "\n",
        "if not os.path.exists(valid_dir):\n",
        "  os.makedirs(valid_dir)\n",
        "\n",
        "for i in range(0, len(breeds)):\n",
        "  if ids[i] in y_train.to_numpy()[:, 0]:\n",
        "    p = train_dir + breeds[i] + \"/\"\n",
        "    if not os.path.exists(p):\n",
        "      os.makedirs(p)\n",
        "\n",
        "    # save picture at p\n",
        "    copyfile(\"train/train_images/\" + ids[i] + \".jpg\", p + ids[i] + \".jpg\")\n",
        "  else:\n",
        "    p = valid_dir + breeds[i] + \"/\"\n",
        "    if not os.path.exists(p):\n",
        "      os.makedirs(p)\n",
        "    \n",
        "    # save picture at p\n",
        "    copyfile(\"train/train_images/\" + ids[i] + \".jpg\", p + ids[i] + \".jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIURpBxAiGh5",
        "colab_type": "text"
      },
      "source": [
        "__Train famous Alexnet model on Dog breeds dataset. It is not easy to train the alexnet model from \n",
        "scratch on the Dog breeds data itself. Curious minds can try for once to train Alexnet from scratch. We adopt Transfer Learning here. We \n",
        "obtain a pretrained Alexnet model trained on Imagenet and apply transfer learning to it to get better results.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_LuDSCiGh-",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Qysw21iGiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "382df385-684e-4817-e506-0c9e9fd47645"
      },
      "source": [
        "#####################################################################################################\n",
        "## TO DO! :  Freeze the weigths of the pretrained alexnet model and change the last classification ##\n",
        "## layer from 1000 classes of Imagenet to 120 classes of Dog Breeds, only classification layer     ##\n",
        "## should be unfreezed and trainable                                                               ##\n",
        "#####################################################################################################\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "\n",
        "from AlexNet import AlexNet_imagenet\n",
        "\n",
        "pretrained_alexnet = models.alexnet(pretrained=True)\n",
        "pretrained_modules = pretrained_alexnet.modules()\n",
        "wts = []\n",
        "for i, j in enumerate(pretrained_modules):\n",
        "  if i in [1, 17, 18, 19, 20, 21, 22]:\n",
        "    wts += [copy.deepcopy(j.state_dict())]\n",
        "\n",
        "# best_model_wts = copy.deepcopy(pretrained_alexnet.state_dict())\n",
        "\n",
        "new_alexnet = AlexNet_imagenet(num_classes=120)\n",
        "new_modules = new_alexnet.modules()\n",
        "for i, j in enumerate(new_modules):\n",
        "  if i is 1:\n",
        "    j.load_state_dict(wts[0])\n",
        "  elif i is 16:\n",
        "    j.load_state_dict(wts[1])\n",
        "  elif i is 17:\n",
        "    j.load_state_dict(wts[2])\n",
        "  elif i is 18:\n",
        "    j.load_state_dict(wts[3])\n",
        "  elif i is 19:\n",
        "    j.load_state_dict(wts[4])\n",
        "  elif i is 20:\n",
        "    j.load_state_dict(wts[5])\n",
        "  elif i is 21:\n",
        "    j.load_state_dict(wts[6])\n",
        "\n",
        "# Below function will directly train your network with the given parameters to 5 epochs\n",
        "# You are also free to use function learned in task 1 to train your model here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define train_loader and val_loader of\n",
        "trainloader = load_dataset('converted/train/')\n",
        "validloader = load_dataset('converted/valid/')\n",
        "new_alexnet.cuda()\n",
        "\n",
        "train_loss, test_loss = start_train_test(new_alexnet, trainloader, validloader, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=> Training Epoch #1, LR=0.0100\n",
            "| Epoch [  1/  5] \t\tLoss: 4.9038 Acc@1: 0.709%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 4.7907 Acc@1: 0.88%\n",
            "* Test results : Acc@1 = 0.88%\n",
            "| Elapsed time : 0:01:04\n",
            "\n",
            "=> Training Epoch #2, LR=0.0100\n",
            "| Epoch [  2/  5] \t\tLoss: 44.7649 Acc@1: 1.015%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 4.7953 Acc@1: 1.03%\n",
            "* Test results : Acc@1 = 1.03%\n",
            "| Elapsed time : 0:02:08\n",
            "\n",
            "=> Training Epoch #3, LR=0.0100\n",
            "| Epoch [  3/  5] \t\tLoss: 5.1042 Acc@1: 0.783%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 4.7692 Acc@1: 1.03%\n",
            "* Test results : Acc@1 = 1.03%\n",
            "| Elapsed time : 0:03:12\n",
            "\n",
            "=> Training Epoch #4, LR=0.0100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p67dZcFFiGiX",
        "colab_type": "text"
      },
      "source": [
        "## Making Kaggle Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-i7wgiJiGih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################################################################\n",
        "## Not So optimal Code: This can take upto 2 minutes to run: You are free to make an optimal version :)     ##\n",
        "## It iterates over all test images to compute the softmax probablities from the last layer of the network  ##\n",
        "##############################################################################################################\n",
        "from transform import transform_testing\n",
        "import PIL.Image\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "augment_image = transform_testing()\n",
        "test_data_root = 'data/dog_breeds/test/' \n",
        "test_image_list = os.listdir(test_data_root) # list of test files \n",
        "result = []\n",
        "for img_name in test_image_list:\n",
        "    img = PIL.Image.open(test_data_root + img_name)\n",
        "    img_tensor = augment_image(img)\n",
        "    with torch.no_grad():\n",
        "        output = pretrained_resnet(img_tensor.unsqueeze_(0).cuda())\n",
        "        probs = F.softmax(output, dim=1)\n",
        "    result.append(probs.cpu().numpy())\n",
        "\n",
        "all_predictions = np.concatenate(result)\n",
        "print(all_predictions.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-2iLnIiGjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(all_predictions)\n",
        "file_list = os.listdir('data/dog_data_imagenet/train') # list of classes to be provided here\n",
        "df.columns = sorted(file_list)\n",
        "\n",
        "# insert clean ids - without folder prefix and .jpg suffix - of images as first column\n",
        "test_data_root = 'data/dog_breeds/test/' # list of all test files here\n",
        "test_image_list = os.listdir(test_data_root)\n",
        "df.insert(0, \"id\", [e[:-4] for e in test_image_list])\n",
        "df.to_csv(f\"sub_1_alexnet.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBaXZCDiGjT",
        "colab_type": "text"
      },
      "source": [
        "### TO DO!: ###\n",
        "Submit the created CSV file to Kaggle, with a score(cross entropy loss) not more than __2.0__\\\n",
        "Take a snapshot of your rank on Kaggle Public Leaderboard and include the image here ...\n",
        "For example :\n",
        "![title](https://github.com/Gruppe3VDL/Gruppe3VDLws2019/blob/master/snp2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-ak6cIiGjY",
        "colab_type": "text"
      },
      "source": [
        "## CHALLENGE  (optional)\n",
        "Compete against each other, Come up with creative ideas. Try beating the score of __0.3__. The group with minimum score gets a small prize at the time when the solutions are discussed. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugz9Z0hiiGjg",
        "colab_type": "text"
      },
      "source": [
        "__Hints:__\n",
        "\n",
        "1. Instead of Alexnet use pretrained resnet 18 model for better accuracy\n",
        "2. Instead of a just adding the last classification layer, try adding two layers to get a better loss\n",
        "3. Train some more layers at the end of the network with a very very small learning rate\n",
        "4. Add Batch Normalizations or Dropout to the layers you have added, (If not present)\n",
        "5. Add more augmentation to your dataset, see tranform.py file and use auto autoaugment to apply more rigorous data augmentation techniques"
      ]
    }
  ]
}