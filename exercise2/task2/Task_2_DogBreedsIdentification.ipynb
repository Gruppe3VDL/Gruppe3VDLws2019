{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Task_2_DogBreedsIdentification_saif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gruppe3VDL/Gruppe3VDLws2019/blob/master/exercise2/task2/Task_2_DogBreedsIdentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uJd6k4jiGcr",
        "colab_type": "text"
      },
      "source": [
        "__Complete all sub-tasks marked with ## TO DO! ## and submit the filled notebook on OLAT__ \\\n",
        "__Using a GPU is recommended here__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTk2h0lXiGdI",
        "colab_type": "text"
      },
      "source": [
        "### Transfer Learning ###\n",
        "Aim of this notebook is to implement the concept of transfer learning to train a bigger dataset. We try to compete on a well-known competiton on Kaggle known as Dog Breeds Identification. Read more about it here:\n",
        "\n",
        "https://www.kaggle.com/c/dog-breed-identification/overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPlCpQl1iGdT",
        "colab_type": "text"
      },
      "source": [
        "To train a model on the Dog breeds dataset using transfer learning and submit your results to Kaggle.\n",
        "Note: Below notebook gives some tips to run the code in pytorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuiPm2ADiGdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpC1eJ-YiGfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## Register on Kaggle with your respepective group name                                  ##\n",
        "##                                                                                       ##\n",
        "##                                                                                       ##\n",
        "##         Group Name:  WS19_VDL_GROUP_03                                                ##\n",
        "##        Kaggle Page:  https://www.kaggle.com/ws19vdlgroup03                            ##\n",
        "##                                                                                       ##\n",
        "##                                                                                       ##\n",
        "###########################################################################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ8VLTQBiGf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## Download the Dog-Breeds dataset in folder \"data\" from the Kaggle competition link     ##\n",
        "## mentioned above.                                                                      ##\n",
        "###########################################################################################\n",
        "#!mkdir /root/.kaggle\n",
        "#!echo '{\"username\":\"ws19vdlgroup03\",\"key\":\"820848a6160b4d9eeb38e21cb8ba74c3\"}' > /root/.kaggle/kaggle.json\n",
        "#!pip install kaggle\n",
        "#!kaggle competitions download -c dog-breed-identification\n",
        "#!unzip 'train.zip'\n",
        "#!unzip 'test.zip'\n",
        "#!unzip 'labels.csv.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEQs2oPIedzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## We saved the downloaded dataset in Google Drive. Here, we define the paths to data    ##\n",
        "## which can then be used in this notebook on Google Colab                               ##\n",
        "###########################################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ow-F9iRdH_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Navigate to project location\n",
        "%cd '/content/drive/My Drive/TUK/Very Deep Learning/Exercises/exercise2/task2/'\n",
        "\n",
        "# Define dataset locations\n",
        "dir_root = \"data/\"\n",
        "dir_train = dir_root + \"train/\"\n",
        "dir_test = dir_root + \"test/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCOlYJr4jCcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "\n",
        "# def getListOfFiles(dirName):\n",
        "#   # create a list of file and sub directories \n",
        "#   # names in the given directory \n",
        "#   listOfFile = os.listdir(dirName)\n",
        "#   allFiles = list()\n",
        "\n",
        "#   # Iterate over all the entries\n",
        "#   for entry in listOfFile:\n",
        "#     # Create full path\n",
        "#     fullPath = os.path.join(dirName, entry)\n",
        "\n",
        "#     # If entry is a directory then get the list of files in this directory \n",
        "#     if os.path.isdir(fullPath):\n",
        "#       allFiles = allFiles + getListOfFiles(fullPath)\n",
        "#     else:\n",
        "#       allFiles.append(fullPath)\n",
        "              \n",
        "#   return allFiles\n",
        "\n",
        "# seen = []\n",
        "# duplicates = []\n",
        "\n",
        "# im = getListOfFiles(\"data/test/\")\n",
        "# for i in im:\n",
        "#   if \"(1)\" in i:\n",
        "#     duplicates += [i]\n",
        "#   else:\n",
        "#     seen += [i]\n",
        "\n",
        "# print(len(seen))\n",
        "# print(len(duplicates))\n",
        "\n",
        "# # Delete all duplicates\n",
        "# for i in duplicates:\n",
        "#   os.remove(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEoxTjhiGgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## Make your dataset to and dataloaders for the  test data                               ##\n",
        "###########################################################################################\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Define data loader\n",
        "def load_dataset(data_path):\n",
        "    dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transforms.Compose([torchvision.transforms.Resize((224, 224)),\n",
        "                                      torchvision.transforms.ToTensor()]))\n",
        "    dataset = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=256,\n",
        "        num_workers=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Load test data\n",
        "test_dataset = load_dataset(dir_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v91dtTDiGgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## Split train data into 20% validation set and make dataloaders for train and val split ##\n",
        "###########################################################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load train data and labels\n",
        "train_dataset = load_dataset(dir_train)\n",
        "train_labels = pd.read_csv(dir_root + 'labels.csv')\n",
        "\n",
        "# Split labels using 80/20 ratio\n",
        "y_train, y_validation = train_test_split(train_labels, test_size=0.20, random_state=42)\n",
        "\n",
        "# Print shapes for verification\n",
        "print(\"Train Set\", y_train.shape)\n",
        "print(\"Validation Set\", y_validation.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aY7kJ0MiGhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## HINT: One can (1) make their own custom dataset and dataloaders using the CSV file or ##\n",
        "## (2) convert the Dog-breed training dataset into Imagenet Format, where all images of  ##\n",
        "## one class are in a folder named with class as in the below given format. Standard     ##\n",
        "## Pytorch Datasets and Dataloaders can then be applied over them                        ##\n",
        "##   Root                                                                                ##\n",
        "##   |                                                                                   ##\n",
        "##   |---Class1 ___Img1.png                                                              ##\n",
        "##   |          ___Img2.png                                                              ##\n",
        "##   |                                                                                   ##\n",
        "##   |---Class2 ___Img3.png                                                              ##\n",
        "##   |          ___Img4.png                                                              ##\n",
        "##   |....                                                                               ##\n",
        "##   |....                                                                               ##\n",
        "###########################################################################################\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "# We are going for option (2) given in hint above, by manually converting whole training\n",
        "# dataset to Imagenet Format. Two separate datasets in Imagenet Format from the training\n",
        "# and validation split above are created. Not very optimal, takes ages to run.\n",
        "dir_converted_train = dir_root + \"converted/train/\"\n",
        "dir_converted_valid = dir_root + \"converted/valid/\"\n",
        "\n",
        "ids = train_labels['id'].to_numpy()\n",
        "breeds = train_labels['breed'].to_numpy()\n",
        "\n",
        "if not os.path.exists(dir_converted_train):\n",
        "  os.makedirs(dir_converted_train)\n",
        "\n",
        "if not os.path.exists(dir_converted_valid):\n",
        "  os.makedirs(dir_converted_valid)\n",
        "\n",
        "print(\"Total Images:\", len(breeds))\n",
        "for i in range(0, len(breeds)):\n",
        "  path = dir_converted_train + breeds[i] + \"/\"\n",
        "  if ids[i] in y_validation.to_numpy()[:, 0]:\n",
        "    path = dir_converted_valid + breeds[i] + \"/\"\n",
        "  \n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  src = dir_train + \"train_images/\" + ids[i] + \".jpg\"\n",
        "  out = path + ids[i] + \".jpg\"\n",
        "  # copyfile(src, out) # UNCOMMENT THIS LINE TO CONVERT DATASET\n",
        "  print(\"\\rConverted: {}%\".format(round(i/len(breeds) * 100, 2)), end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIURpBxAiGh5",
        "colab_type": "text"
      },
      "source": [
        "__Train famous Alexnet model on Dog breeds dataset. It is not easy to train the alexnet model from \n",
        "scratch on the Dog breeds data itself. Curious minds can try for once to train Alexnet from scratch. We adopt Transfer Learning here. We \n",
        "obtain a pretrained Alexnet model trained on Imagenet and apply transfer learning to it to get better results.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_LuDSCiGh-",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Qysw21iGiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "## Freeze the weigths of the pretrained alexnet model and change the last classification ##\n",
        "## layer from 1000 classes of Imagenet to 120 classes of Dog Breeds, only classification ##\n",
        "## layer should be unfreezed and trainable                                               ##\n",
        "###########################################################################################\n",
        "import torchvision.models as models\n",
        "\n",
        "from train_test import start_train_test\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class AlexNet():\n",
        "  def __init__(self, num_classes=120, inputs=3):\n",
        "    # Use PyTorch's pretrained AlexNet\n",
        "    self.pretrained_model = models.alexnet(pretrained=True)\n",
        "\n",
        "    # Freeze weights on all layers\n",
        "    for name, param in self.pretrained_model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace last layer (new layer has requires_grad=True by default)\n",
        "    in_params = self.pretrained_model.classifier[6].in_features\n",
        "    self.pretrained_model.classifier[6] = nn.Linear(in_params, num_classes)\n",
        "    self.pretrained_model.cuda()\n",
        "\n",
        "class ResNet18():\n",
        "  def __init__(self, num_classes=120, inputs=3):\n",
        "    # Use PyTorch's pretrained AlexNet\n",
        "    self.pretrained_model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Freeze weights on all layers\n",
        "    for name, param in self.pretrained_model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace last layer (new layer has requires_grad=True by default)\n",
        "    in_params = self.pretrained_model.fc.in_features\n",
        "    self.pretrained_model.fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(in_params, in_params)),\n",
        "                                            # ('dr1_1', nn.Dropout(p=0.5)),\n",
        "                                            ('bn1_1', nn.BatchNorm1d(in_params)),\n",
        "                                            ('fc2', nn.Linear(in_params, 512)),\n",
        "                                            ('bn2_1', nn.BatchNorm1d(512)),\n",
        "                                            ('fc3', nn.Linear(512, 256)),\n",
        "                                            ('bn3_1', nn.BatchNorm1d(256)),\n",
        "                                            ('fc4', nn.Linear(256, num_classes))]))\n",
        "    self.pretrained_model.cuda()\n",
        "\n",
        "class GoogleNet():\n",
        "  def __init__(self, num_classes=120, inputs=3):\n",
        "    # Use PyTorch's pretrained AlexNet\n",
        "    self.pretrained_model = models.googlenet(pretrained=True)\n",
        "\n",
        "    # Freeze weights on all layers\n",
        "    for name, param in self.pretrained_model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace last layer (new layer has requires_grad=True by default)\n",
        "    in_params = self.pretrained_model.fc.in_features\n",
        "    self.pretrained_model.fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(in_params, 512)),\n",
        "                                            # ('dr1_1', nn.Dropout(p=0.5)),\n",
        "                                            ('bn1_1', nn.BatchNorm1d(512)),\n",
        "                                            ('fc2', nn.Linear(512, 256)),\n",
        "                                            ('bn2_1', nn.BatchNorm1d(256)),\n",
        "                                            ('fc4', nn.Linear(256, num_classes))]))\n",
        "    self.pretrained_model.cuda()\n",
        "\n",
        "# net = AlexNet().pretrained_model\n",
        "# net = ResNet18().pretrained_model\n",
        "net = GoogleNet().pretrained_model\n",
        "\n",
        "# Create training and validation dataloaders\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainloader = load_dataset(dir_converted_train)\n",
        "validloader = load_dataset(dir_converted_valid)\n",
        "\n",
        "# Below function will directly train your network with the given parameters to 5 epochs\n",
        "# You are also free to use function learned in task 1 to train your model here\n",
        "train_loss, test_loss = start_train_test(net, trainloader, validloader, criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p67dZcFFiGiX",
        "colab_type": "text"
      },
      "source": [
        "## Making Kaggle Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-i7wgiJiGih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################################################################\n",
        "## Not So optimal Code: This can take upto 2 minutes to run: You are free to make an optimal version :)     ##\n",
        "## It iterates over all test images to compute the softmax probablities from the last layer of the network  ##\n",
        "##############################################################################################################\n",
        "from transform import transform_testing\n",
        "import PIL.Image\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "augment_image = transform_testing()\n",
        "test_data_root = dir_test + '/test_images/'\n",
        "test_image_list = os.listdir(test_data_root) # list of test files \n",
        "result = []\n",
        "for img_name in test_image_list:\n",
        "    img = PIL.Image.open(test_data_root + img_name)\n",
        "    img_tensor = augment_image(img)\n",
        "    with torch.no_grad():\n",
        "        output = net(img_tensor.unsqueeze_(0).cuda())\n",
        "        probs = F.softmax(output, dim=1)\n",
        "    result.append(probs.cpu().numpy())\n",
        "\n",
        "all_predictions = np.concatenate(result)\n",
        "print(all_predictions.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-2iLnIiGjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(all_predictions)\n",
        "file_list = os.listdir(dir_converted_train) # list of classes to be provided here\n",
        "df.columns = sorted(file_list)\n",
        "\n",
        "# insert clean ids - without folder prefix and .jpg suffix - of images as first column\n",
        "test_data_root = dir_test + 'test_images/' # list of all test files here\n",
        "test_image_list = os.listdir(test_data_root)\n",
        "df.insert(0, \"id\", [e[:-4] for e in test_image_list])\n",
        "df.to_csv(f\"sub_1_alexnet.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBaXZCDiGjT",
        "colab_type": "text"
      },
      "source": [
        "### TO DO!: ###\n",
        "Submit the created CSV file to Kaggle, with a score(cross entropy loss) not more than __2.0__\\\n",
        "Take a snapshot of your rank on Kaggle Public Leaderboard and include the image here ...\n",
        "For example :\n",
        "![title](https://github.com/Gruppe3VDL/Gruppe3VDLws2019/blob/master/snp2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-ak6cIiGjY",
        "colab_type": "text"
      },
      "source": [
        "## CHALLENGE  (optional)\n",
        "Compete against each other, Come up with creative ideas. Try beating the score of __0.3__. The group with minimum score gets a small prize at the time when the solutions are discussed. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugz9Z0hiiGjg",
        "colab_type": "text"
      },
      "source": [
        "__Hints:__\n",
        "\n",
        "1. Instead of Alexnet use pretrained resnet 18 model for better accuracy\n",
        "2. Instead of a just adding the last classification layer, try adding two layers to get a better loss\n",
        "3. Train some more layers at the end of the network with a very very small learning rate\n",
        "4. Add Batch Normalizations or Dropout to the layers you have added, (If not present)\n",
        "5. Add more augmentation to your dataset, see tranform.py file and use auto autoaugment to apply more rigorous data augmentation techniques"
      ]
    }
  ]
}